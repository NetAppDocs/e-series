---
permalink: config-linux/task_nvme_over_roce_express_setup.html
sidebar: sidebar
keywords: 
summary: ''
---
= NVMe over RoCE Express Setup
:experimental:
:icons: font
:imagesdir: ../media/

[.lead]
You can use NVMe with the RDMA over Converged Ethernet (RoCE) network protocol.

== Verify the Linux configuration is supported

[.lead]
To ensure reliable operation, you create an implementation plan and then use the NetApp Interoperability Matrix Tool (IMT) to verify that the entire configuration is supported.

. Go to the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].
. Click on the *Solution Search* tile.
. In the menu:Protocols[SAN Host] area, click the *Add* button next to *E-Series SAN Host*.
. Click *View Refine Search Criteria*.
+
The *Refine Search Criteria* section is displayed. In this section you may select the protocol that applies, as well as other criteria for the configuration such as Operating System, NetApp OS, and Host Multipath driver. Select the criteria you know you want for your configuration, and then see what compatible configuration elements apply. As necessary, make the updates for your operating system and protocol that are prescribed in the tool. Detailed information for your chosen configuration is accessible on the View Supported Configurations page by clicking the *right page arrow*.

=== NVMe over RoCE restrictions

[.lead]
Before using NVMe over RoCE, review the controller, host, and recovery restrictions.

==== Verify your configuration

Verify your configuration, using the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].

==== About the hardware

NVME over RoCE can be configured for the EF300 (100GB controllers only), EF600, EF570, and E5700 controllers. The controllers must have 100GB or 200GB host port.

==== Restrictions

The following restrictions are in effect for the 11.60 release. See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of requirements.

==== Controller restrictions

* This protocol can be used only for EF300, EF600, EF570, and E5700 controllers. A minimum of 32 GB of physical memory is required to use the protocol on EF600, EF570, and E5700 controllers. For the EF300, a minimum of 16 GB of physical memory is required. If the minimum memory requirements for the controllers are not met during start of day operations, a message is displayed that helps you diagnose the problem.
* No simplex (single-controller) configurations are supported.
* The only supported host interface card (HIC) is the 100G or 200G EDR HIC, which also supports NVMe over InfiniBand, iSER and SRP.
* There is no support for mixed NVMe over RoCE with NVMe over InfiniBand or SCSI host interfaces.

==== Switch restrictions

IMPORTANT: *RISK OF DATA LOSS.* You must enable Priority Flow Control or Global Pause Control on the switch to eliminate the risk of data loss in an NVMe over RoCE environment.

==== Host, host protocol, and host operating system restrictions

* See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of requirements.
* For a list of supported host channel adapters see the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] .

==== Storage and disaster recovery restrictions

* Asynchronous and synchronous mirroring are not supported.
* Thin provisioning (the creation of thin volumes) is not supported.

== Configuring IP addresses using DHCP

[.lead]
In this express method for configuring communications between the management station and the storage array, you use Dynamic Host Configuration Protocol (DHCP) to provide IP addresses. Each storage array has either one controller (simplex) or two controllers (duplex), and each controller has two storage management ports. Each management port will be assigned an IP address.

You have installed and configured a DHCP server on the same subnet as the storage management ports.

The following instructions refer to a storage array with two controllers (a duplex configuration).

. If you have not already done so, connect an Ethernet cable to the management station and to management port 1 on each controller (A and B).
+
The DHCP server assigns an IP address to port 1 of each controller.
+
NOTE: Do not use management port 2 on either controller. Port 2 is reserved for use by NetApp technical personnel.
+
IMPORTANT: If you disconnect and reconnect the Ethernet cable, or if the storage array is power-cycled, DHCP assigns IP addresses again. This process occurs until static IP addresses are configured. It is recommended that your avoid disconnecting the cable or power-cycling the array.
+
If the storage array cannot get DHCP-assigned IP addresses within 30 seconds, the following default IP addresses are set:

 ** Controller A, port 1: 169.254.128.101
 ** Controller B, port 1: 169.254.128.102
 ** Subnet mask: 255.255.0.0

. Locate the MAC address label on the back of each controller, and then provide your network administrator with the MAC address for port 1 of each controller.
+
Your network administrator needs the MAC addresses to determine the IP address for each controller. You will need the IP addresses to connect to your storage system through your browser.

== Install SANtricity Storage Manager for SMcli (SANtricity software version 11.53 or earlier)

[.lead]
When you install the SANtricity Storage Manager software on your management station, a graphical user interface (GUI) and a command line interface (CLI) are installed by default. These instructions assume that you will install the SANtricity Storage Manager GUI on a management station and _not_ on the I/O host.

IMPORTANT: For SANtricity software 11.60 and newer, the SANtricity Secure CLI (SMcli) is included in the SANtricity OS and downloadable through the SANtricity System Manager. For more information on how to download the SMcli through the SANtricty System Manager, refer to the _Download the command line interface (CLI)_ topic under the SANtricity System Manager Online Help.

* You are using SANtricity software 11.53 or earlier.
* You must have the correct administrator or superuser privileges.
* You must have ensured that the system that will contain the SANtricity Storage Manager client has the following minimum requirements:
 ** *RAM*: 2 GB for Java Runtime Engine
 ** *Disk space*: 5 GB
 ** *OS/Architecture*: Refer to https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for guidance on determining the supported operating system versions and architectures.

This section describes how to install SANtricity Storage Manager on both the Windows and Linux OS platforms, because both Windows and Linux are common management station platforms when Linux is used for the data host.

. Download the SANtricity software release from https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager*.

== Access SANtricity System Manager and use Setup wizard

[.lead]
You use the Setup wizard in SANtricity System Manager to configure your storage array.

* You have ensured that the device from which you will access SANtricity System Manager contains one of the following browsers:
+
|===
| Browser| Minimum version
a|
Google Chrome
a|
47
a|
Microsoft Internet Explorer
a|
11
a|
Microsoft Edge
a|
EdgeHTML 12
a|
Mozilla Firefox
a|
31
a|
Safari
a|
9
|===

* You are using out-of-band management.

The wizard automatically relaunches when you open System Manager or refresh your browser and _at least one_ of the following conditions is met:

* No pools and volume groups are detected.
* No workloads are detected.
* No notifications are configured.

. From your browser, enter the following URL: `https://<DomainNameOrIPAddress>`
+
IPAddress is the address for one of the storage array controllers.
+
The first time SANtricity System Manager is opened on an array that has not been configured, the Set Administrator Password prompt appears. Role-based access management configures four local roles: admin, support, security, and monitor. The latter three roles have random passwords that cannot be guessed. After you set a password for the admin role you can change all of the passwords using the admin credentials. See _SANtricity System Manager online help_ for more information on the four local user roles.

. Enter the System Manager password for the admin role in the Set Administrator Password and Confirm Password fields, and then select the *Set Password* button.
+
When you open System Manager and no pools, volumes groups, workloads, or notifications have been configured, the Setup wizard launches.

. Use the Setup wizard to perform the following tasks:
 ** *Verify hardware (controllers and drives)* -- Verify the number of controllers and drives in the storage array. Assign a name to the array.
 ** *Verify hosts and operating systems* -- Verify the host and operating system types that the storage array can access.
 ** *Accept pools* -- Accept the recommended pool configuration for the express installation method. A pool is a logical group of drives.
 ** *Configure alerts* -- Allow System Manager to receive automatic notifications when a problem occurs with the storage array.
 ** *Enable AutoSupport* -- Automatically monitor the health of your storage array and have dispatches sent to technical support.
. If you have not already created a volume, create one by going to *Storage* > *Volumes* > *Create* > *Volume*.
+
For more information, see the online help for SANtricity System Manager.

== Configure the switch

[.lead]
You configure the switches according to the vendor's recommendations for NVMe over RoCE. These recommendations might include both configuration directives as well as code updates.

IMPORTANT: *RISK OF DATA LOSS.* You must enable Priority Flow Control or Global Pause Control on the switch to eliminate the risk of data loss in an NVMe over RoCE environment.

Enable Ethernet pause frame flow control *end to end* as the best practice configuration.

Consult your network administrator for tips on selecting the best configuration for your environment.

== Set up NVMe over RoCE on the host side

[.lead]
NVMe initiator configuration in a RoCE environment includes installing and configuring the rdma-core and nvme-cli packages, configuring initiator IP addresses, and setting up the NVMe-oF layer on the host.

* You are running RHEL 7 and the latest compatible SUSE Linux Enterprise Server 12 and 15 service pack operating system. See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of the latest requirements.

. Install the rdma and nvme-cli packages:
+
----

# zypper install rdma-core
# zypper install nvme-cli
----
+
*RHEL 7*
+
----

# yum install rdma-core
# yum install nvme-cli
----

. Setup IPv4 IP addresses on the ethernet ports used to connect NVMe over RoCE. For each network interface, create a configuration script that contains the different variables for that interface.
+
The variables used in this step are based on server hardware and the network environment. The variables include the IPADDR and GATEWAY. These are example instructions for the latest SUSE Linux Enterprise Server 12 service pack:
+
Create the example file/etc/sysconfig/network/ifcfg-eth4 as follows:
+
----
BOOTPROTO='static'
   BROADCAST=
   ETHTOOL_OPTIONS=
   IPADDR='192.168.1.87/24'
   GATEWAY='192.168.1.1'
   MTU=
   NAME='MT27800 Family [ConnectX-5]'
   NETWORK=
   REMOTE_IPADDR=
   STARTMODE='auto'
----
+
Create the second example file/etc/sysconfig/network/ifcfg-eth5 as follows:
+
----
BOOTPROTO='static'
   BROADCAST=
   ETHTOOL_OPTIONS=
   IPADDR='192.168.2.87/24'
   GATEWAY='192.168.2.1'
   MTU=
   NAME='MT27800 Family [ConnectX-5]'
   NETWORK=
   REMOTE_IPADDR=
   STARTMODE='auto'
----

. Enable the network interfaces:
+
----

# ifup eth4
# ifup eth5
----

. Set up the NVMe-oF layer on the host.
 .. Create the following file under /etc/modules-load.d/ to load the `nvme-rdma` kernel module and make sure the kernel module will always be on, even after a reboot:
+
----

# cat /etc/modules-load.d/nvme-rdma.conf
  nvme-rdma
----

== Configure storage array NVMe over RoCE connections

[.lead]
If your controller includes a connection for NVMe over RoCE (RDMA over Converged Ethernet), you can configure the NVMe port settings from the Hardware page or the System page in SANtricity System Manager.

* Your controller must include an NVMe over RoCE host port; otherwise, the NVMe over RoCE settings are not available in System Manager.
* You must know the IP address of the host connection.

You can access the NVMe over RoCE configuration from the *Hardware* page or from menu:Settings[System]. This task describes how to configure the ports from the Hardware page.

NOTE: The NVMe over RoCE settings and functions appear only if your storage array's controller includes an NVMe over RoCE port.

. Select *Hardware*.
. Click the controller with the NVMe over RoCE port you want to configure.
+
The controller's context menu appears.

. Select *Configure NVMe over RoCE ports*.
+
The *Configure NVMe over RoCE ports* dialog box opens.

. In the drop-down list, select the port you want to configure, and then click *Next*.

== Discover and connect to the storage from the host

[.lead]
Before making definitions of each host in SANtricity System Manager, you must discover the target controller ports from the host, and then establish NVMe connections.

. Discover available subsystems on the NVMe-oF target for all paths using the following command:
+
----
nvme discover -t rdma -a target_ip_address
----
+
In this command, target_ip_address is the IP address of the target port.
+
NOTE: The `nvme discover` command discovers all controller ports in the subsystem, regardless of host access.
+
----
# nvme discover -t rdma -a 192.168.1.77
Discovery Log Number of Records 2, Generation counter 0
=====Discovery Log Entry 0======
trtype:  rdma
adrfam:  ipv4
subtype: nvme subsystem
treq:    not specified
portid:  0
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:5700.600a098000a527a7000000005ab3af94
traddr:  192.168.1.77
rdma_prtype: roce
rdma_qptype: connected
rdma_cms:    rdma-cm
rdma_pkey: 0x0000
=====Discovery Log Entry 1======
trtype:  rdma
adrfam:  ipv4
subtype: nvme subsystem
treq:    not specified
portid:  1
trsvcid: 4420
subnqn:  nqn.1992-08.com.netapp:5700.600a098000a527a7000000005ab3af94
traddr:  192.168.2.77
rdma_prtype: roce
rdma_qptype: connected
rdma_cms:    rdma-cm
rdma_pkey: 0x0000
----

. Repeat step 1 for any other connections.
. Connect to the discovered subsystem on the first path using the command: ``nvme connect -t rdma -n``discovered_sub_nqn``-a``target_ip_address``-Q``queue_depth_setting``-l``controller_loss_timeout_period
+
NOTE: The `nvme connect -t rdma -n discovered_sub_nqn -a target_ip_address -Q queue_depth_setting -l controller_loss_timeout_period` command does not persist through reboot. The NVMe connect command will need to executed after each reboot to re-establish the NVMe connections.
+
IMPORTANT: Connections are not established for any discovered port inaccessible by the host.
+
IMPORTANT: If you specify a port number using this command, the connection fails. The default port is the only port set up for connections.
+
IMPORTANT: The recommended queue depth setting is 1024. Override the default setting of 128 with 1024 using the ``-Q 1024``command line option, as shown in the following example.
+
IMPORTANT: The recommended controller loss timeout period in seconds is 60 minutes (3600 seconds). Override the default setting of 600 seconds with 3600 seconds using the `-l 3600` command line option, as shown in the following example.
+
----

# nvme connect -t rdma -a 192.168.1.77 -n nqn.1992-08.com.netapp:5700.600a098000a527a7000000005ab3af94 -Q 1024 -l 3600
# nvme connect -t rdma -a 192.168.2.77 -n nqn.1992-08.com.netapp:5700.600a098000a527a7000000005ab3af94 -Q 1024 -l 3600
----

. Repeat step 3 to connect the discovered subsystem on the second path.

== Define a host

[.lead]
Using SANtricity System Manager, you define the hosts that send data to the storage array. Defining a host is one of the steps required to let the storage array know which hosts are attached to it and to allow I/O access to the volumes.

Keep these guidelines in mind when you define a host:

* You must define the host identifier ports that are associated with the host.
* Make sure that you provide the same name as the host's assigned system name.
* This operation does not succeed if the name you choose is already in use.
* The length of the name cannot exceed 30 characters.

. Select *Storage* > *Hosts*.
. Click *Create* > *Host*.
+
The Create Host dialog box appears.

. Select the settings for the host as appropriate.
+
Field details
+
|===
| Setting| Description
a|
Name
a|
Type a name for the new host.
a|
Host operating system type
a|
Select one of the following options from the drop-down list:

 ** *SANtricity 11.60 and newer*
+
Linux

 ** *Pre SANtricity 11.60*
+
Linux DM-MP (Kernel 3.10 or later)

a|
Host interface type
a|
Select the host interface type that you want to use. If the array you configure only has one available host interface type, this setting may not be available to select.
a|
Host ports
a|
Do one of the following:

 ** *Select I/O Interface*
+
If the host ports have logged in, you can select host port identifiers from the list. This is the recommended method.

 ** *Manual add*
+
If the host ports have not logged in, look at `/etc/nvme/hostnqn` on the host to find the hostnqn identifiers and associate them with the host definition.
+
You can manually enter the host port identifiers or copy/paste them from the `/etc/nvme/hostnqn` file (one at a time) into the *Host ports* field.
+
You must add one host port identifier at a time to associate it with the host, but you can continue to select as many identifiers that are associated with the host. Each identifier is displayed in the *Host ports* field. If necessary, you also can remove an identifier by selecting the *X* next to it.

+
|===

. Click *Create*.

After the host is successfully created, SANtricity System Manager creates a default name for each host port configured for the host.

The default alias is <Hostname_Port Number>. For example, the default alias for the first port created for host IPT is IPT_1.

== Assign a volume

[.lead]
You must assign a volume (namespace) to a host or host cluster so it can be used for I/O operations. This assignment grants a host or host cluster access to one or more namespaces in a storage array.

Keep these guidelines in mind when you assign volumes:

* You can assign a volume to only one host or host cluster at a time.
* Assigned volumes are shared between controllers in the storage array.
* The same namespace ID (NSID) cannot be used twice by a host or a host cluster to access a volume. You must use a unique NSID.

Assigning a volume fails under these conditions:

* All volumes are assigned.
* The volume is already assigned to another host or host cluster.

The ability to assign a volume is unavailable under these conditions:

* No valid hosts or host clusters exist.
* All volume assignments have been defined.

All unassigned volumes are displayed, but functions for hosts with or without Data Assurance (DA) apply as follows:

* For a DA-capable host, you can select volumes that are either DA-enabled or not DA-enabled.
* For a host that is not DA-capable, if you select a volume that is DA-enabled, a warning states that the system must automatically turn off DA on the volume before assigning the volume to the host.

. Select *Storage* > *Hosts*.

After successfully assigning a volume or volumes to a host or a host cluster, the system performs the following actions:

* The assigned volume receives the next available NSID. The host uses the NSID to access the volume.
* The user-supplied volume name appears in volume listings associated to the host.

== Display the volumes visible to the host

[.lead]
The SMdevices tool, part of the nvme-cli package, allows you to view the volumes currently visible on the host. This tool is an alternative to the `nvme list` command.

. To view information about each NVMe path to an E-Series volume, use thenvme netapp smdevices [-o <format>] command. The output <format> can be normal (the default if -o is not used), column, or json.
+
----
# nvme netapp smdevices
/dev/nvme1n1, Array Name ICTM0706SYS04, Volume Name NVMe2, NSID 1, Volume ID 000015bd5903df4a00a0980000af4462, Controller A, Access State unknown, 2.15GB
/dev/nvme1n2, Array Name ICTM0706SYS04, Volume Name NVMe3, NSID 2, Volume ID 000015c05903e24000a0980000af4462, Controller A, Access State unknown, 2.15GB
/dev/nvme1n3, Array Name ICTM0706SYS04, Volume Name NVMe4, NSID 4, Volume ID 00001bb0593a46f400a0980000af4462, Controller A, Access State unknown, 2.15GB
/dev/nvme1n4, Array Name ICTM0706SYS04, Volume Name NVMe6, NSID 6, Volume ID 00001696593b424b00a0980000af4112, Controller A, Access State unknown, 2.15GB
/dev/nvme2n1, Array Name ICTM0706SYS04, Volume Name NVMe2, NSID 1, Volume ID 000015bd5903df4a00a0980000af4462, Controller B, Access State unknown, 2.15GB
/dev/nvme2n2, Array Name ICTM0706SYS04, Volume Name NVMe3, NSID 2, Volume ID 000015c05903e24000a0980000af4462, Controller B, Access State unknown, 2.15GB
/dev/nvme2n3, Array Name ICTM0706SYS04, Volume Name NVMe4, NSID 4, Volume ID 00001bb0593a46f400a0980000af4462, Controller B, Access State unknown, 2.15GB
/dev/nvme2n4, Array Name ICTM0706SYS04, Volume Name NVMe6, NSID 6, Volume ID 00001696593b424b00a0980000af4112, Controller B, Access State unknown, 2.15GB
----

== Set up failover on the host

[.lead]
Multipath software provides a redundant path to the storage array in case one of the physical paths is disrupted. There are currently two methods of multipathing available for NVMe, and which you will be using is going to be dependent on which OS version you are running. For SLES 12, device mapper multipath (DMMP) will be used. For RHEL 7 and SLES 15, a native NVMe multipathing solution will be used.

=== Configure the host to run failover

[.lead]
The SUSE Linux Enterprise Server host requires configuration changes to run failover. The failover solution uses DM-MP.

* You have installed the required packages on your system.
* For Red Hat (RHEL) hosts, verify the packages are installed by running `rpm -q device-mapper-multipath`
* For SLES hosts, verify the packages are installed by running `rpm -q multipath-tools`
+
NOTE: Refer to the NetApp Interoperability Matrix Tool (IMT) to ensure any required updates are installed as multipathing may not work correctly with the GA versions of SLES or RHEL.

By default, DM-MP is disabled in RHWL and SLES. Complete the following steps to enable DM-MP components on the host.

. Add the NVMe E-Series device entry to the devices section of the `/etc/multipath.conf` file, as shown in the following example:
+
----

devices {
        device {
                vendor "NVME"
                product "NetApp E-Series*"
                path_grouping_policy group_by_prio
                failback immediate
                no_path_retry 30
        }
}
----

== Access NVMe volumes for virtual device targets

[.lead]
You can configure the I/O directed to the device target based on your Linux version. For RHEL 7 and SLES 12, I/O is directed to virtual device targets by the Linux host. DM-MP manages the physical paths underlying these virtual targets.

=== Virtual devices are I/O targets

Make sure you are running I/O only to the virtual devices created by DM-MP and not to the physical device paths. If you are running I/O to the physical paths, DM-MP cannot manage a failover event and the I/O fails.

You can access these block devices through the `dm` device or the `symlink` in `/dev/mapper`, for example:

----
/dev/dm-1
/dev/mapper/eui.00001bc7593b7f5f00a0980000af4462
----

=== Example

The following example output from the `nvme list` command shows the host node name and its correlation with the namespace ID.

----

NODE         SN           MODEL           NAMESPACE

/dev/nvme1n1 021648023072 NetApp E-Series 10
/dev/nvme1n2 021648023072 NetApp E-Series 11
/dev/nvme1n3 021648023072 NetApp E-Series 12
/dev/nvme1n4 021648023072 NetApp E-Series 13
/dev/nvme2n1 021648023151 NetApp E-Series 10
/dev/nvme2n2 021648023151 NetApp E-Series 11
/dev/nvme2n3 021648023151 NetApp E-Series 12
/dev/nvme2n4 021648023151 NetApp E-Series 13
----

|===
| Column| Description
a|
`Node`

a|
The node name includes two parts:

* The notation `nvme1` represents controller A and `nvme2` represents controller B.
* The notation `n1`, `n2`, and so on represent the namespace identifier from the host perspective. These identifiers are repeated in the table, once for controller A and once for controller B.

a|
`Namespace`

a|
The Namespace column lists the namespace ID (NSID), which is the identifier from the storage array perspective.

|===
In the following `multipath -ll` output, the optimized paths are shown with a `prio` value of 50, while the non-optimized paths are shown with a `prio` value of 10.

The Linux operating system routes I/O to the path group that is shown as `status=active`, while the path groups listed as `status=enabled` are available for failover.

----
eui.00001bc7593b7f500a0980000af4462 dm-0 NVME,NetApp E-Series
size=15G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| `- #:#:#:# nvme1n1 259:5 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  `- #:#:#:# nvme2n1 259:9  active ready running

eui.00001bc7593b7f5f00a0980000af4462 dm-0 NVME,NetApp E-Series
size=15G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=0 status=enabled
| `- #:#:#:# nvme1n1 259:5 failed faulty running
`-+- policy='service-time 0' prio=10 status=active
  `- #:#:#:# nvme2n1 259:9  active ready running
----

|===
| Line item| Description
a|
`policy='service-time 0' prio=50 status=active`

a|
This line and the following line show that `nvme1n1`, which is the namespace with an NSID of 10, is optimized on the path with a `prio` value of 50 and a `status` value of `active`.

This namespace is owned by controller A.

a|
`policy='service-time 0' prio=10 status=enabled`

a|
This line shows the failover path for namespace 10, with a `prio` value of 10 and a `status` value of `enabled`. I/O is not being directed to the namespace on this path at the moment.

This namespace is owned by controller B.

a|
`policy='service-time 0' prio=0 status=enabled`

a|
This example shows ``multipath -ll``output from a different point in time, while controller A is rebooting. The path to namespace 10 is shown as `failed faulty running` with a `prio` value of 0 and a `status` value of `enabled`.

a|
`policy='service-time 0' prio=10 status=active`

a|
Note that the `active` path refers to `nvme2`, so the I/O is being directed on this path to controller B.

|===

== Accessing NVMe volumes for physical NVMe device targets

[.lead]
You can configure the I/O directed to the device target based on your Linux version. For SLES 15, I/O is directed to the physical NVMe device targets by the Linux host. A native NVMe multipathing solution manages the physical paths underlying the single apparent physical device displayed by the host.

NOTE: It is best practice to use the links in /dev/disk/by-id/ rather than /dev/nvme0n1, for example:

----
# ls /dev/disk/by-id/ -l lrwxrwxrwx 1 root root 13 Oct 18 15:14
nvme-
eui.0000320f5cad32cf00a0980000af4112 -> ../../nvme0n1
----

=== Physical NVMe devices are I/O targets

Run I/O to the physical nvme device path. There should only be one of these devices present for each namespace using the following format:

----
/dev/nvme[subsys#]n[id#]
----

All paths are virtualized using the native multipathing solution underneath this device.

You can view your paths by running:

----
# nvme list-subsys
----

Example output:

----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:5700.600a098000a522500000000589aa8a6
\
+- nvme0 rdma traddr=192.4.21.131 trsvcid=4420 live
+- nvme1 rdma traddr=192.4.22.141 trsvcid=4420 live
----

If you specify a namespace device when using the 'nvme list-subsys' command, it provides additional information about the paths to that namespace:

----
# nvme list-subsys /dev/nvme0n1
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:5700.600a098000af44620000000058d5dd96
\
 +- nvme0 rdma traddr=192.168.130.101 trsvcid=4420 live non-optimized
 +- nvme1 rdma traddr=192.168.131.101 trsvcid=4420 live non-optimized
 +- nvme2 rdma traddr=192.168.130.102 trsvcid=4420 live optimized
 +- nvme3 rdma traddr=192.168.131.102 trsvcid=4420 live optimized
----

There are also hooks into the multipath commands to allow you to view your path information for native failover through them as well:

----
#multipath -ll
----

NOTE: To view the path information, the following must be set in `/etc/multipath.conf`:

----

defaults {
        enable_foreign nvme
}
----

Example output:

----
eui.0000a0335c05d57a00a0980000a5229d [nvme]:nvme0n9 NVMe,Netapp E-Series,08520001
size=4194304 features='n/a' hwhandler='ANA' wp=rw
|-+- policy='n/a' prio=50 status=optimized
| `- 0:0:1 nvme0c0n1 0:0 n/a optimized    live
`-+- policy='n/a' prio-10 status=non-optimized
`- 0:1:1 nvme0c1n1 0:0 n/a non-optimized    live
----

== Create filesystems (RHEL 7 and SLES 12)

[.lead]
For RHEL 7 and SLES 12, you create a file system on the namespace and mount the filesystem.

. Run the multipath -ll command to get a list of/dev/mapper/dm devices.
+
----
# multipath -ll
----
+
The result of this command shows two devices, dm-19 and dm-16 :
+
----
eui.00001ffe5a94ff8500a0980000af4444 dm-19 NVME,NetApp E-Series
size=10G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- #:#:#:# nvme0n19 259:19  active ready running
| `- #:#:#:# nvme1n19 259:115 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- #:#:#:# nvme2n19 259:51  active ready running
  `- #:#:#:# nvme3n19 259:83  active ready running
eui.00001fd25a94fef000a0980000af4444 dm-16 NVME,NetApp E-Series
size=16G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- #:#:#:# nvme0n16 259:16  active ready running
| `- #:#:#:# nvme1n16 259:112 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- #:#:#:# nvme2n16 259:48  active ready running
  `- #:#:#:# nvme3n16 259:80  active ready running
----

== Create filesystems (SLES 15)

[.lead]
For SLES 15, you create a filesystem on the native nvme device and mount the filesystem.

. Run the multipath -ll command to get a list of /dev/nvme devices.
+
----
# multipath -ll
----
+
The result of this command shows device nvme0n6.
+
----
eui.000082dd5c05d39300a0980000a52225 [nvme]:nvme0n6 NVMe,NetApp E-Series,08520000
size=4194304 features='n/a' hwhandler='ANA' wp=rw
|-+- policy='n/a' prio=50 status=optimized
| `- 0:0:1 nvme0c0n1 0:0 n/a optimized     live
|-+- policy='n/a' prio=50 status=optimized
| `- 0:1:1 nvme0c1n1 0:0 n/a optimized     live
|-+- policy='n/a' prio=10 status=non-optimized
| `- 0:2:1 nvme0c2n1 0:0 n/a non-optimized live
`-+- policy='n/a' prio=10 status=non-optimized
  `- 0:3:1 nvme0c3n1 0:0 n/a non-optimized live
----

== Verify storage access on the host

[.lead]
Before using the namespace, you verify that the host can write data to the namespace and read it back.

. On the host, copy one or more files to the mount point of the disk.

You remove the file and folder that you copied.

== NVMe over RoCE worksheet for Linux

[.lead]
You can use this worksheet to record NVMe over RoCE storage configuration information. You need this information to perform provisioning tasks.

=== Direct connect topology

In a direct connect topology, one or more hosts are directly connected to the subsystem. In the SANtricity OS 11.50 release, we support a single connection from each host to a subsystem controller, as shown below. In this configuration, one HCA (host channel adapter) port from each host should be on the same subnet as the E-Series controller port it is connected to, but on a different subnet from the other HCA port.

image::../media/nvmeof_direct_connect.gif[]

An example configuration that satisfies the requirements consists of four network subnets as follows:

* Subnet 1: Host 1 HCA Port 1 and Controller 1 Host port 1
* Subnet 2: Host 1 HCA Port 2 and Controller 2 Host port 1
* Subnet 3: Host 2 HCA Port 1 and Controller 1 Host port 2
* Subnet 4: Host 2 HCA Port 2 and Controller 2 Host port 2

=== Switch connect topology

In a fabric topology, one or more switches are used. Refer to https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a list of supported switches.

image::../media/nvmeof_switch_connect.gif[]

=== NVMe over RoCE: Host identifiers

Locate and document the initiator NQN from each host.

|===
| Host port connections| Software initiator NQN
a|
Host (initiator) 1
a|
 
a|
 
a|
 
a|
Host (initiator) 2
a|
 
a|
 
a|
 
a|
 
a|
 
|===

=== NVMe over RoCE: Target NQN

Document the target NQN for the storage array.

|===
| Array name| Target NQN
a|
Array controller (target)
a|
 
|===

=== NVMe over RoCE: Target NQNs

Document the NQNs to be used by the array ports.

|===
| Array controller (target) port connections| NQN
a|
Controller A, port 1
a|
 
a|
Controller B, port 1
a|
 
a|
Controller A, port 2
a|
 
a|
Controller B, port 2
a|
 
|===

=== NVMe over RoCE: Mapping host name

NOTE: The mapping host name is created during the workflow.

|===
a|
Mapping host name
a|
 
a|
Host OS type
a|
 
|===
