---
permalink: config-linux/task_nvme_over_infiniband_express_setup.html
sidebar: sidebar
keywords: 
summary: ''
---
= NVMe over InfiniBand Express Setup
:experimental:
:icons: font
:imagesdir: ../media/

[.lead]
You can use NVMe with the InfiniBand network protocol.

== Verify the Linux configuration is supported

[.lead]
To ensure reliable operation, you create an implementation plan and then use the NetApp Interoperability Matrix Tool (IMT) to verify that the entire configuration is supported.

. Go to the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].
. Click on the *Solution Search* tile.
. In the menu:Protocols[SAN Host] area, click the *Add* button next to *E-Series SAN Host*.
. Click *View Refine Search Criteria*.
+
The *Refine Search Criteria* section is displayed. In this section you may select the protocol that applies, as well as other criteria for the configuration such as Operating System, NetApp OS, and Host Multipath driver. Select the criteria you know you want for your configuration, and then see what compatible configuration elements apply. As necessary, make the updates for your operating system and protocol that are prescribed in the tool. Detailed information for your chosen configuration is accessible on the View Supported Configurations page by clicking the *right page arrow*.

=== NVMe over InfiniBand restrictions

[.lead]
Before using NVMe over InfiniBand, review the controller, host, and recovery restrictions.

==== Verify your configuration

Verify your configuration, using the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].

==== About the hardware

NVMe over InfiniBand can be configured for EF300 (100GB controllers only), EF600, EF570, or E5700 controllers. The controllers must have 100GB or 200GB InfiniBand host ports.

==== Restrictions

The following restrictions are in effect for the 11.60 release. See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of requirements.

==== Controller restrictions

* This protocol can be used only for EF300, EF600, EF570, or EF570 controllers. A minimum of 32 GB of physical memory is required to use this protocol on EF600, EF570, and E5700 controllers. For the EF300, a minimum of 16 GB of physical memory is required. If the minimum memory requirements for the controllers are not met during start of day operations, a message is displayed that helps you diagnose the problem.
* No simplex (single controller) configurations are supported.
* There is no support for mixed NVMe over InfiniBand and SCSI host interfaces.
* For EF300 controllers, no more than 64 NVMe hosts can be supported on the IB interface.

==== Host, host protocol and host operating system restrictions

* The host must be running the latest compatible RHEL 7, SUSE Linux Enterprise Server 12 or 15 service pack operating system . See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of the latest requirements.
* The only supported host channel adapters are from Mellanox. See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for more information.
* The only supported host interface card (HIC) is the 100G or 200G EDR IB HIC, which also supports iSER and SRP (but iSER and SRP are not supported simultaneously).

==== Storage and disaster recovery restrictions

* Asynchronous and synchronous mirroring are not supported.
* Thin provisioning (the creation of thin volumes) is not supported.

== Configure IP addresses using DHCP

[.lead]
In this express method for configuring communications between the management station and the storage array, you use Dynamic Host Configuration Protocol (DHCP) to provide IP addresses. Each storage array has either one controller (simplex) or two controllers (duplex), and each controller has two storage management ports. Each management port will be assigned an IP address.

You have installed and configured a DHCP server on the same subnet as the storage management ports.

The following instructions refer to a storage array with two controllers (a duplex configuration).

. If you have not already done so, connect an Ethernet cable to the management station and to management port 1 on each controller (A and B).
+
The DHCP server assigns an IP address to port 1 of each controller.
+
NOTE: Do not use management port 2 on either controller. Port 2 is reserved for use by NetApp technical personnel.
+
IMPORTANT: If you disconnect and reconnect the Ethernet cable, or if the storage array is power-cycled, DHCP assigns IP addresses again. This process occurs until static IP addresses are configured. It is recommended that your avoid disconnecting the cable or power-cycling the array.
+
If the storage array cannot get DHCP-assigned IP addresses within 30 seconds, the following default IP addresses are set:

 ** Controller A, port 1: 169.254.128.101
 ** Controller B, port 1: 169.254.128.102
 ** Subnet mask: 255.255.0.0

. Locate the MAC address label on the back of each controller, and then provide your network administrator with the MAC address for port 1 of each controller.
+
Your network administrator needs the MAC addresses to determine the IP address for each controller. You will need the IP addresses to connect to your storage system through your browser.

== Install SANtricity Storage Manager for SMcli (SANtricity software version 11.53 or earlier)

[.lead]
When you install the SANtricity Storage Manager software on your management station, a graphical user interface (GUI) and a command line interface (CLI) are installed by default. These instructions assume that you will install the SANtricity Storage Manager GUI on a management station and _not_ on the I/O host.

IMPORTANT: For SANtricity software 11.60 and newer, the SANtricity Secure CLI (SMcli) is included in the SANtricity OS and downloadable through the SANtricity System Manager. For more information on how to download the SMcli through the SANtricty System Manager, refer to the _Download the command line interface (CLI)_ topic under the SANtricity System Manager Online Help.

* You are using SANtricity software 11.53 or earlier.
* You must have the correct administrator or superuser privileges.
* You must have ensured that the system that will contain the SANtricity Storage Manager client has the following minimum requirements:
 ** *RAM*: 2 GB for Java Runtime Engine
 ** *Disk space*: 5 GB
 ** *OS/Architecture*: Refer to https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager* for guidance on determining the supported operating system versions and architectures.

This section describes how to install SANtricity Storage Manager on both the Windows and Linux OS platforms, because both Windows and Linux are common management station platforms when Linux is used for the data host.

. Download the SANtricity software release from https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager*.

== Access SANtricity System Manager and use the Setup wizard

[.lead]
You use the Setup wizard in SANtricity System Manager to configure your storage array.

* You have ensured that the device from which you will access SANtricity System Manager contains one of the following browsers:
+
|===
| Browser| Minimum version
a|
Google Chrome
a|
47
a|
Microsoft Internet Explorer
a|
11
a|
Microsoft Edge
a|
EdgeHTML 12
a|
Mozilla Firefox
a|
31
a|
Safari
a|
9
|===

* You are using out-of-band management.

The wizard automatically relaunches when you open System Manager or refresh your browser and _at least one_ of the following conditions is met:

* No pools and volume groups are detected.
* No workloads are detected.
* No notifications are configured.

. From your browser, enter the following URL: `https://<DomainNameOrIPAddress>`
+
IPAddress is the address for one of the storage array controllers.
+
The first time SANtricity System Manager is opened on an array that has not been configured, the Set Administrator Password prompt appears. Role-based access management configures four local roles: admin, support, security, and monitor. The latter three roles have random passwords that cannot be guessed. After you set a password for the admin role you can change all of the passwords using the admin credentials. See _SANtricity System Manager online help_ for more information on the four local user roles.

. Enter the System Manager password for the admin role in the Set Administrator Password and Confirm Password fields, and then select the *Set Password* button.
+
When you open System Manager and no pools, volumes groups, workloads, or notifications have been configured, the Setup wizard launches.

. Use the Setup wizard to perform the following tasks:
 ** *Verify hardware (controllers and drives)* -- Verify the number of controllers and drives in the storage array. Assign a name to the array.
 ** *Verify hosts and operating systems* -- Verify the host and operating system types that the storage array can access.
 ** *Accept pools* -- Accept the recommended pool configuration for the express installation method. A pool is a logical group of drives.
 ** *Configure alerts* -- Allow System Manager to receive automatic notifications when a problem occurs with the storage array.
 ** *Enable AutoSupport* -- Automatically monitor the health of your storage array and have dispatches sent to technical support.
. If you have not already created a volume, create one by going to *Storage* > *Volumes* > *Create* > *Volume*.
+
For more information, see the online help for SANtricity System Manager.

== Configure subnet manager

[.lead]
Using an InfiniBand switch to run subnet manager might cause unexpected path loss during high loads. To avoid path loss, configure the subnet manager on one or more of your hosts using opensm.

* You are running the latest compatible RHEL 7, SUSE Linux Enterprise Server 12 or 15 service pack operating system. See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of the latest requirements.

. Use the ``ibstat -p``command to find `GUID0` and `GUID1` of the HCA ports. For example:
+
----
# ibstat -p
 0x248a070300a80a80
 0x248a070300a80a81
----

. The way that you configure Subnet Manager depends on your configuration:
 ** If you are using a single switch, start and enable the `opensm` service, then add the HCA port identifier values you found in link:task_nvme_over_infiniband_express_setup.md#STEP_7F5F9B74260F4842B83D82184CB1EC48[step 2] to the `opensm.conf` file on each port. Repeat for the other port.
  *** Edit the `/etc/rdma/opensm.conf` file to add the identifier for that port:
+
----
opensm -c /etc/rdma/opensm.conf

# The port GUID on which the OpenSM is running
guid 0x248a070300a80a80
----
 ** If you are using the direct connect method, or if you have multiple switches, enable Subnet Manager on each port of the connected HCA on the host:
  *** Add the following two lines to `/etc/rc.d/after.local` (for SUSE Linux Enterprise Server 12 and SLES 15 service pack ). Substitute the values you found in link:task_nvme_over_infiniband_express_setup.md#STEP_7F5F9B74260F4842B83D82184CB1EC48[step 2] for `GUID0` and `GUID1`. For `P0` and `P1`, use the subnet manager priorities, with 1 being the lowest and 15 the highest:
+
*SLES example*
+
----
 opensm -B -g GUID0 -p P0 -f /var/log/opensm-ib0.log
 opensm -B -g GUID1 -p P1 -f /var/log/opensm-ib1.log
----
+
An example of the command with value substitutions.
+
----
# cat /etc/rc.d/rc.local
 opensm -B -g 0x248a070300a80a80 -p 15 -f /var/log/opensm-ib0.log
 opensm -B -g 0x248a070300a80a81 -p 1 -f /var/log/opensm-ib1.log
----

  *** Add the following two lines to `/etc/rc.d/rc.local` (for RHEL 7 ).Substitute the values you found in link:task_nvme_over_infiniband_express_setup.md#STEP_7F5F9B74260F4842B83D82184CB1EC48[step 2] for `GUID0` and `GUID1`. For `P0` and `P1`, use the subnet manager priorities, with 1 being the lowest and 15 the highest:
+
*RHEL example*
+
----
 opensm -B -g GUID0 -p P0 -f /var/log/opensm-ib0.log
 opensm -B -g GUID1 -p P1 -f /var/log/opensm-ib1.log
----
+
An example of the command with value substitutions.
+
----
# cat /etc/rc.d/rc.local
 opensm -B -g 0x248a070300a80a80 -p 15 -f /var/log/opensm-ib0.log
 opensm -B -g 0x248a070300a80a81 -p 1 -f /var/log/opensm-ib1.log
----

== Set up NVMe over InfiniBand on the host side

[.lead]
Configuring an NVMe initiator in an InfiniBand environment includes installing and configuring the infiniband, nvme-cli, and rdma packages, configuring initiator IP addresses, and setting up the NVMe-oF layer on the host.

* You are running the latest compatible RHEL 7, SUSE Linux Enterprise Server 12 and 15 service pack operating system. See the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for a complete list of the latest requirements.

. Install the rdma, nvme-cli, and infiniband packages:
+
----

# zypper install infiniband-diags
# zypper install rdma-core
# zypper install nvme-cli
----
+
*RHEL 7*
+
----

# yum install infiniband-diags
# yum install rdma-core
# yum install nvme-cli
----

. Enable ipoib. Edit the `/etc/rdma/rdma.conf` file and modify the entry for loading ipoib:
+
----
IPOIB_LOAD=yes
----

. Check that both ib port links are up and the State = Active:
+
----
 # ibstat
----
+
----
CA 'mlx4_0'
        CA type: MT4099
        Number of ports: 2
        Firmware version: 2.40.7000
        Hardware version: 1
        Node GUID: 0x0002c90300317850
        System image GUID: 0x0002c90300317853
        Port 1:
                State: Active
                Physical state: LinkUp
                Rate: 40
                Base lid: 4
                LMC: 0
                SM lid: 4
                Capability mask: 0x0259486a
                Port GUID: 0x0002c90300317851
                Link layer: InfiniBand
        Port 2:
                State: Active
                Physical state: LinkUp
                Rate: 56
                Base lid: 5
                LMC: 0
                SM lid: 4
                Capability mask: 0x0259486a
                Port GUID: 0x0002c90300317852
                Link layer: InfiniBand
----

. Set up IPv4 IP addresses on the ib ports.
+
For SUSE Linux Enterprise Server 12 and 15, create the file /etc/sysconfig/network/ifcfg-ib0
+
----

  BOOTPROTO='static'
  BROADCAST=
  ETHTOOL_OPTIONS=
  IPADDR='10.10.10.100/24'
  IPOIB_MODE='connected'
  MTU='65520'
  NAME=
  NETWORK=
  REMOTE_IPADDR=
  STARTMODE='auto'
----
+
Then, create the file /etc/sysconfig/network/ifcfg-ib1.
+
----

  BOOTPROTO='static'
  BROADCAST=
  ETHTOOL_OPTIONS=
  IPADDR='11.11.11.100/24'
  IPOIB_MODE='connected'
  MTU='65520'
  NAME=
  NETWORK=
  REMOTE_IPADDR=
  STARTMODE='auto'
----
+
For RHEL, create the file /etc/sysconfig/network-scripts/ifcfg-ib0:
+
----

  CONNECTED_MODE=no
  TYPE=InfiniBand
  PROXY_METHOD=none
  BROWSER_ONLY=no
  BOOTPROTO=static
  IPADDR='10.10.10.100/24'
  DEFROUTE=no
  IPV4=FAILURE_FATAL=yes
  IPV6INIT=no
  NAME=ib0
  ONBOOT=yes
----
+
Then, create the file /etc/sysconfig/network-scripts/ifcfg-ib1:.
+
----

  CONNECTED_MODE=no
  TYPE=InfiniBand
  PROXY_METHOD=none
  BROWSER_ONLY=no
  BOOTPROTO=static
  IPADDR='11.11.11.100/24'
  DEFROUTE=no
  IPV4=FAILURE_FATAL=yes
  IPV6INIT=no
  NAME=ib1
  ONBOOT=yes
----

. Enable the `ib` interface:
+
----

# ifup ib0
# ifup ib1
----

. Verify the IP addresses you will use to connect to the array. Run this command for both `ib0` and `ib1`:
+
----

# ip addr show ib0
# ip addr show ib1
----
+
As shown in the example below, the IP address for `ib0` is `10.10.10.255`.
+
----
10: ib0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65520 qdisc pfifo_fast state UP group default qlen 256
    link/infiniband 80:00:02:08:fe:80:00:00:00:00:00:00:00:02:c9:03:00:31:78:51 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    inet 10.10.10.255 brd 10.10.10.255 scope global ib0
       valid_lft forever preferred_lft forever
    inet6 fe80::202:c903:31:7851/64 scope link
       valid_lft forever preferred_lft forever
----
+
As shown in the example below, the IP address for `ib1` is `11.11.11.255`.
+
----
10: ib1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65520 qdisc pfifo_fast state UP group default qlen 256
    link/infiniband 80:00:02:08:fe:80:00:00:00:00:00:00:00:02:c9:03:00:31:78:51 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    inet 11.11.11.255 brd 11.11.11.255 scope global ib0
       valid_lft forever preferred_lft forever
    inet6 fe80::202:c903:31:7851/64 scope link
       valid_lft forever preferred_lft forever
----

. Set up the NVMe-oF layer on the host.
 .. Create the following files under /etc/modules-load.d/ to load the `nvme-rdma` kernel module and make sure the kernel module will always be on, even after a reboot:
+
----

# cat /etc/modules-load.d/nvme-rdma.conf
  nvme-rdma
----

== Configure storage array NVMe over InfiniBand connections

[.lead]
If your controller includes an NVMe over InfiniBand port, you can configure the IP address of each port using SANtricity System Manager.

. Select *Hardware*.
. If the graphic shows the drives, click *Show back of shelf*.
+
The graphic changes to show the controllers instead of the drives.

. Click the controller with the NVMe over InfiniBand ports you want to configure.
+
The controller's context menu appears.

. Select *Configure NVMe over InfiniBand ports*.
+
NOTE: The Configure NVMe over InfiniBand ports option appears only if System Manager detects NVMe over InfiniBand ports on the controller.
+
The *Configure NVMe over InfiniBand Ports* dialog box opens.

. In the drop-down list, select the HIC port you want to configure, and then enter the IP address of the port.
. Click *Configure*.
. Repeat steps 5 and 6 for the other HIC ports that will be used.

== Discover and connect to the storage from the host

[.lead]
Before making definitions of each host in SANtricity System Manager, you must discover the target controller ports from the host, and then establish NVMe connections.

. Discover available subsystems on the NVMe-oF target for all paths using the following command:
+
----
nvme discover -t rdma -a target_ip_address
----
+
In this command, target_ip_address is the IP address of the target port.
+
NOTE: The `nvme discover` command discovers all controller ports in the subsystem, regardless of host access.
+
----
# nvme discover  -t rdma -a 10.10.10.100
 Discovery Log Number of Records 2, Generation counter 0
 =====Discovery Log Entry 0======
 trtype:  rdma
 adrfam:  ipv4
 subtype: nvme subsystem
 treq:    not specified
 portid:  0
 trsvcid: 4420
 subnqn:  nqn.1992-08.com.netapp:5700.600a098000af41580000000058ed54be
 traddr: 10.10.10.100
 rdma_prtype: infiniband
 rdma_qptype: connected
 rdma_cms:    rdma-cm
 rdma_pkey: 0x0000
=====Discovery Log Entry 1======
 trtype:  rdma
 adrfam:  ipv4
 subtype: nvme subsystem
 treq:    not specified
 portid:  1
 trsvcid: 4420
 subnqn:  nqn.1992-08.com.netapp:5700.600a098000af41580000000058ed54be
 traddr: 11.11.11.100
 rdma_prtype: infiniband
 rdma_qptype: connected
 rdma_cms:    rdma-cm
 rdma_pkey: 0x0000
----

. Repeat step 1 for any other connections.
. Connect to the discovered subsystem on the first path using the command: ``nvme connect -t rdma -n``discovered_sub_nqn``-a``target_ip_address``-Q``queue_depth_setting``-l``controller_loss_timeout_period
+
NOTE: The `nvme connect -t rdma -n discovered_sub_nqn -a target_ip_address -Q queue_depth_setting -l controller_loss_timeout_period` command does not persist through reboot. The NVMe connect command will need to executed after each reboot to re-establish the NVMe connections.
+
NOTE: The nvme connections do not persist through system reboot or extended periods of the controller being unavailable.
+
IMPORTANT: Connections are not established for any discovered port inaccessible by the host.
+
IMPORTANT: If you specify a port number using this command, the connection fails. The default port is the only port set up for connections.
+
IMPORTANT: The recommended queue depth setting is 1024. Override the default setting of 128 with 1024 using the ``-Q 1024``command line option, as shown in the following example.
+
IMPORTANT: The recommended controller loss timeout period in seconds is 60 minutes (3600 seconds). Override the default setting of 600 seconds with 3600 seconds using the `-l 3600` command line option, as shown in the following example.
+
----
  # nvme connect -t rdma -a 10.10.10.100 -n  nqn.1992-08.com.netapp:5700.600a098000af41580000000058ed54be -Q 1024 -l 3600
----

. Use the `nvme list` command to see a list of the NVMe devices currently connected. In the example below, it is `nvme0n1`.
+
----
 # nvme list

 Node          SN            Model             Namespace
 -------------------------------------------------------
 /dev/nvme0n1  021648023161  NetApp E-Series      1
----
+
----
Usage                 Format           FW Rev
--------------------------------------------------------------
5.37 GB /5.37 GB      512 B + 0 B      0842XXXX
----

. Connect to the discovered subsystem on the second path:
+
----
  # nvme connect -t rdma -a 11.11.11.100 -n  nqn.1992-08.com.netapp:5700.600a098000af41580000000058ed54be -Q 1024 -l 3600
----

. Use the Linux lsblk and grep commands to show additional information about each block device:
+
----
 # lsblk | grep nvme

 nvme0n1    259:0    0     5G  0 disk
 nvme1n1    259:0    0     5G  0 disk
----

. Use the `nvme list` command to see a new list of the NVMe devices currently connected. In the example below, it is `nvme0n1` and `nvme0n1`.
+
----
 # nvme list
 Node          SN            Model                   Namespace
 -------------------------------------------------------------
 /dev/nvme0n1  021648023161  NetApp E-Series          1
 /dev/nvme1n1  021648023161  NetApp E-Series          1
----
+
----
Usage                 Format           FW Rev
--------------------------------------------------------------
5.37 GB /5.37 GB          512 B + 0 B      0842XXXX
5.37 GB /5.37 GB          512 B + 0 B      0842XXXX
----

== Define a host

[.lead]
Using SANtricity System Manager, you define the hosts that send data to the storage array. Defining a host is one of the steps required to let the storage array know which hosts are attached to it and to allow I/O access to the volumes.

Keep these guidelines in mind when you define a host:

* You must define the host identifier ports that are associated with the host.
* Make sure that you provide the same name as the host's assigned system name.
* This operation does not succeed if the name you choose is already in use.
* The length of the name cannot exceed 30 characters.

. Select *Storage* > *Hosts*.

After the host is successfully created, SANtricity System Manager creates a default name for each host port configured for the host.

The default alias is <Hostname_Port Number>. For example, the default alias for the first port created for host IPT is IPT_1.

== Assign a volume

[.lead]
You must assign a volume (namespace) to a host or host cluster so it can be used for I/O operations. This assignment grants a host or host cluster access to one or more namespaces in a storage array.

Keep these guidelines in mind when you assign volumes:

* You can assign a volume to only one host or host cluster at a time.
* Assigned volumes are shared between controllers in the storage array.
* The same namespace ID (NSID) cannot be used twice by a host or a host cluster to access a volume. You must use a unique NSID.

Assigning a volume fails under these conditions:

* All volumes are assigned.
* The volume is already assigned to another host or host cluster.

The ability to assign a volume is unavailable under these conditions:

* No valid hosts or host clusters exist.
* All volume assignments have been defined.

All unassigned volumes are displayed, but functions for hosts with or without Data Assurance (DA) apply as follows:

* For a DA-capable host, you can select volumes that are either DA-enabled or not DA-enabled.
* For a host that is not DA-capable, if you select a volume that is DA-enabled, a warning states that the system must automatically turn off DA on the volume before assigning the volume to the host.

. Select *Storage* > *Hosts*.

After successfully assigning a volume or volumes to a host or a host cluster, the system performs the following actions:

* The assigned volume receives the next available NSID. The host uses the NSID to access the volume.
* The user-supplied volume name appears in volume listings associated to the host.

== Display the volumes visible to the host

[.lead]
Use the SMdevices tool, part of the nvme-cli package, to view the volumes currently visible on the host. This is an alternative to the `nvme list` command.

. To view information about each NVMe path to an E-Series volume, use thenvme netapp smdevices [-o <format>] command. The output <format> can be normal (the default if -o is not used), column, or json.
+
----
# nvme netapp smdevices
/dev/nvme1n1, Array Name ICTM0706SYS04, Volume Name NVMe2, NSID 1, Volume ID 000015bd5903df4a00a0980000af4462, Controller A, Access State unknown, 2.15GB
/dev/nvme1n2, Array Name ICTM0706SYS04, Volume Name NVMe3, NSID 2, Volume ID 000015c05903e24000a0980000af4462, Controller A, Access State unknown, 2.15GB
/dev/nvme1n3, Array Name ICTM0706SYS04, Volume Name NVMe4, NSID 4, Volume ID 00001bb0593a46f400a0980000af4462, Controller A, Access State unknown, 2.15GB
/dev/nvme1n4, Array Name ICTM0706SYS04, Volume Name NVMe6, NSID 6, Volume ID 00001696593b424b00a0980000af4112, Controller A, Access State unknown, 2.15GB
/dev/nvme2n1, Array Name ICTM0706SYS04, Volume Name NVMe2, NSID 1, Volume ID 000015bd5903df4a00a0980000af4462, Controller B, Access State unknown, 2.15GB
/dev/nvme2n2, Array Name ICTM0706SYS04, Volume Name NVMe3, NSID 2, Volume ID 000015c05903e24000a0980000af4462, Controller B, Access State unknown, 2.15GB
/dev/nvme2n3, Array Name ICTM0706SYS04, Volume Name NVMe4, NSID 4, Volume ID 00001bb0593a46f400a0980000af4462, Controller B, Access State unknown, 2.15GB
/dev/nvme2n4, Array Name ICTM0706SYS04, Volume Name NVMe6, NSID 6, Volume ID 00001696593b424b00a0980000af4112, Controller B, Access State unknown, 2.15GB
----

== Set up failover

[.lead]
Multipath software provides a redundant path to the storage array in case one of the physical paths is disrupted. There are currently two methods of multipathing available for NVMe, and which you will be using is going to be dependent on which OS version you are running. For RHEL 7 and SLES 12, device mapper multipath (DMMP) will be used. For SLES 15, a native NVMe multipathing solution will be used.

=== Configure the host to run failover

[.lead]
The SUSE Linux Enterprise Server host requires configuration changes to run failover. The failover solution uses DM-MP.

* You have installed the required packages on your system.
* For Red Hat (RHEL) hosts, verify the packages are installed by running `rpm -q device-mapper-multipath`
* For SLES hosts, verify the packages are installed by running `rpm -q multipath-tools`
+
NOTE: Refer to the NetApp Interoperability Matrix Tool (IMT) to ensure any required updates are installed as multipathing may not work correctly with the GA versions of SLES or RHEL.

By default, DM-MP is disabled in RHWL and SLES. Complete the following steps to enable DM-MP components on the host.

. Add the NVMe E-Series device entry to the devices section of the `/etc/multipath.conf` file, as shown in the following example:
+
----

devices {
        device {
                vendor "NVME"
                product "NetApp E-Series*"
                path_grouping_policy group_by_prio
                failback immediate
                no_path_retry 30
        }
}
----

== Access NVMe volumes for virtual device targets

[.lead]
You can configure the I/O directed to the device target based on your version of SLES (SUSE Linux version). For RHEL 7 and SLES 12, I/O is directed to virtual device targets by the Linux host. DM-MP manages the physical paths underlying these virtual targets.

=== Virtual devices are I/O targets

Make sure you are running I/O only to the virtual devices created by DM-MP and not to the physical device paths. If you are running I/O to the physical paths, DM-MP cannot manage a failover event and the I/O fails.

You can access these block devices through the `dm` device or the `symlink` in `/dev/mapper`, for example:

----
/dev/dm-1
/dev/mapper/eui.00001bc7593b7f5f00a0980000af4462
----

=== Example

The following example output from the `nvme list` command shows the host node name and its correlation with the namespace ID.

----

NODE         SN           MODEL           NAMESPACE

/dev/nvme1n1 021648023072 NetApp E-Series 10
/dev/nvme1n2 021648023072 NetApp E-Series 11
/dev/nvme1n3 021648023072 NetApp E-Series 12
/dev/nvme1n4 021648023072 NetApp E-Series 13
/dev/nvme2n1 021648023151 NetApp E-Series 10
/dev/nvme2n2 021648023151 NetApp E-Series 11
/dev/nvme2n3 021648023151 NetApp E-Series 12
/dev/nvme2n4 021648023151 NetApp E-Series 13
----

|===
| Column| Description
a|
`Node`

a|
The node name includes two parts:

* The notation `nvme1` represents controller A and `nvme2` represents controller B.
* The notation `n1`, `n2`, and so on represent the namespace identifier from the host perspective. These identifiers are repeated in the table, once for controller A and once for controller B.

a|
`Namespace`

a|
The Namespace column lists the namespace ID (NSID), which is the identifier from the storage array perspective.

|===
In the following `multipath -ll` output, the optimized paths are shown with a `prio` value of 50, while the non-optimized paths are shown with a `prio` value of 10.

The Linux operating system routes I/O to the path group that is shown as `status=active`, while the path groups listed as `status=enabled` are available for failover.

----
eui.00001bc7593b7f500a0980000af4462 dm-0 NVME,NetApp E-Series
size=15G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| `- #:#:#:# nvme1n1 259:5 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  `- #:#:#:# nvme2n1 259:9  active ready running

eui.00001bc7593b7f5f00a0980000af4462 dm-0 NVME,NetApp E-Series
size=15G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=0 status=enabled
| `- #:#:#:# nvme1n1 259:5 failed faulty running
`-+- policy='service-time 0' prio=10 status=active
  `- #:#:#:# nvme2n1 259:9  active ready running
----

|===
| Line item| Description
a|
`policy='service-time 0' prio=50 status=active`

a|
This line and the following line show that `nvme1n1`, which is the namespace with an NSID of 10, is optimized on the path with a `prio` value of 50 and a `status` value of `active`.

This namespace is owned by controller A.

a|
`policy='service-time 0' prio=10 status=enabled`

a|
This line shows the failover path for namespace 10, with a `prio` value of 10 and a `status` value of `enabled`. I/O is not being directed to the namespace on this path at the moment.

This namespace is owned by controller B.

a|
`policy='service-time 0' prio=0 status=enabled`

a|
This example shows ``multipath -ll``output from a different point in time, while controller A is rebooting. The path to namespace 10 is shown as `failed faulty running` with a `prio` value of 0 and a `status` value of `enabled`.

a|
`policy='service-time 0' prio=10 status=active`

a|
Note that the `active` path refers to `nvme2`, so the I/O is being directed on this path to controller B.

|===

== Access NVMe volumes for physical NVMe device targets

[.lead]
You can configure the I/O directed to the device target based on your version of SLES (SUSE Linux version). For SLES 15, I/O is directed to the physical NVMe device targets by the Linux host. A native NVMe multipathing solution manages the physical paths underlying the single apparent physical device displayed by the host.

NOTE: It is best practice to use the links in /dev/disk/by-id/ rather than /dev/nvme0n1, for example:

----
# ls /dev/disk/by-id/ -l lrwxrwxrwx 1 root root 13 Oct 18 15:14
nvme-
eui.0000320f5cad32cf00a0980000af4112 -> ../../nvme0n1
----

=== Physical NVMe devices are I/O targets

Run I/O to the physical nvme device path. There should only be one of these devices present for each namespace using the following format:

----
/dev/nvme[subsys#]n[id#]
----

All paths are virtualized using the native multipathing solution underneath this device.

You can view your paths by running:

----
# nvme list-subsys
----

Example output:

----
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:5700.600a098000a522500000000589aa8a6
\
+- nvme0 rdma traddr=192.4.21.131 trsvcid=4420 live
+- nvme1 rdma traddr=192.4.22.141 trsvcid=4420 live
----

If you specify a namespace device when using the 'nvme list-subsys' command, it provides additional information about the paths to that namespace:

----
# nvme list-subsys /dev/nvme0n1
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:5700.600a098000af44620000000058d5dd96
\
 +- nvme0 rdma traddr=192.168.130.101 trsvcid=4420 live non-optimized
 +- nvme1 rdma traddr=192.168.131.101 trsvcid=4420 live non-optimized
 +- nvme2 rdma traddr=192.168.130.102 trsvcid=4420 live optimized
 +- nvme3 rdma traddr=192.168.131.102 trsvcid=4420 live optimized
----

There are also hooks into the multipath commands to allow you to view your path information for native failover through them as well:

----
#multipath -ll
----

NOTE: To view the path information, the following must be set in `/etc/multipath.conf`:

----

defaults {
        enable_foreign nvme
}
----

Example output:

----
eui.0000a0335c05d57a00a0980000a5229d [nvme]:nvme0n9 NVMe,Netapp E-Series,08520001
size=4194304 features='n/a' hwhandler='ANA' wp=rw
|-+- policy='n/a' prio=50 status=optimized
| `- 0:0:1 nvme0c0n1 0:0 n/a optimized    live
`-+- policy='n/a' prio-10 status=non-optimized
`- 0:1:1 nvme0c1n1 0:0 n/a non-optimized    live
----

== Create filesystems (RHEL 7 and SLES 12)

[.lead]
For RHEL 7 and SLES 12, you create a file system on the namespace and mount the filesystem.

. Run the multipath -ll command to get a list of/dev/mapper/dm devices.
+
----
# multipath -ll
----
+
The result of this command shows two devices, dm-19 and dm-16 :
+
----
eui.00001ffe5a94ff8500a0980000af4444 dm-19 NVME,NetApp E-Series
size=10G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- #:#:#:# nvme0n19 259:19  active ready running
| `- #:#:#:# nvme1n19 259:115 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- #:#:#:# nvme2n19 259:51  active ready running
  `- #:#:#:# nvme3n19 259:83  active ready running
eui.00001fd25a94fef000a0980000af4444 dm-16 NVME,NetApp E-Series
size=16G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- #:#:#:# nvme0n16 259:16  active ready running
| `- #:#:#:# nvme1n16 259:112 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- #:#:#:# nvme2n16 259:48  active ready running
  `- #:#:#:# nvme3n16 259:80  active ready running
----

== Create filesystems (SLES 15)

[.lead]
For SLES 15, you create a filesystem on the native nvme device and mount the filesystem.

. Run the multipath -ll command to get a list of /dev/nvme devices.
+
----
# multipath -ll
----
+
The result of this command shows device nvme0n6.
+
----
eui.000082dd5c05d39300a0980000a52225 [nvme]:nvme0n6 NVMe,NetApp E-Series,08520000
size=4194304 features='n/a' hwhandler='ANA' wp=rw
|-+- policy='n/a' prio=50 status=optimized
| `- 0:0:1 nvme0c0n1 0:0 n/a optimized     live
|-+- policy='n/a' prio=50 status=optimized
| `- 0:1:1 nvme0c1n1 0:0 n/a optimized     live
|-+- policy='n/a' prio=10 status=non-optimized
| `- 0:2:1 nvme0c2n1 0:0 n/a non-optimized live
`-+- policy='n/a' prio=10 status=non-optimized
  `- 0:3:1 nvme0c3n1 0:0 n/a non-optimized live
----

== Verify storage access on the host

[.lead]
Before using the namespace, you verify that the host can write data to the namespace and read it back.

You must have initialized the namespace and formatted it with a file system.

. On the host, copy one or more files to the mount point of the disk.

Remove the file and folder that you copied.

== NVMe over InfiniBand worksheet

[.lead]
You can use this worksheet to record NVMe over Infiniband storage configuration information. You need this information to perform provisioning tasks.

=== NVMe over InfiniBand: Host identifiers

NOTE: The software initiator NQN is determined during the task, .

Locate and document the initiator NQN from each host. The NQN is typically found in the /ect/nvme/hostnqn file.

|===
| Callout No.| Host port connections| Host NQN
a|
1
a|
Host (initiator) 1
a|
 
a|
n/a
a|
 
a|
 
a|
n/a
a|
 
a|
 
a|
n/a
a|
 
a|
 
a|
n/a
a|
 
a|
 
|===

=== NVMe over InfiniBand: Recommended configuration

In a direct connect topology, one or more hosts are directly connected to the subsystem. In the SANtricity OS 11.50 release, we support a single connection from each host to a subsystem controller, as shown below. In this configuration, one HCA (host channel adapter) port from each host should be on the same subnet as the E-Series controller port it is connected to, but on a different subnet from the other HCA port.

image::../media/nvmeof_direct_connect.gif[]

=== NVMe over InfiniBand: Target NQN

Document the target NQN for the storage array. You will use this information in link:task_nvme_over_infiniband_express_setup.md#[Configure storage array NVMe over InfiniBand connections] .

Find the Storage Array NQN name using SANtricity: *Storage Array* > *NVMe over Infiniband* > *Manage Settings*. This information might be necessary when you create NVMe over Infiniband sessions from operating systems that do not support send targets discovery.

|===
| Callout No.| Array name| Target IQN
a|
6
a|
Array controller (target)
a|
 
|===

=== NVMe over InfiniBand: Network configuration

Document the network configuration that will be used for the hosts and storage on the InfiniBand fabric. These instructions assume that two subnets will be used for full redundancy.

Your network administrator can provide the following information. You use this information in the topic, link:task_nvme_over_infiniband_express_setup.md#[Configure storage array NVMe over InfiniBand connections].

=== Subnet A

Define the subnet to be used.

|===
| Network Address| Netmask
a|
 
a|
 
|===
Document the NQNs to be used by the array ports and each host port.

|===
| Callout No.| Array controller (target) port connections| NQN
a|
3
a|
Switch
a|
_not applicable_
a|
5
a|
Controller A, port 1
a|
 
a|
4
a|
Controller B, port 1
a|
 
a|
2
a|
Host 1, port 1
a|
 
a|
 
a|
(Optional) Host 2, port 1
a|
 
|===

=== Subnet B

Define the subnet to be used.

|===
| Network Address| Netmask
a|
 
a|
 
|===
Document the IQNs to be used by the array ports and each host port.

|===
| Callout No.| Array controller (target) port connections| NQN
a|
8
a|
Switch
a|
_not applicable_
a|
10
a|
Controller A, port 2
a|
 
a|
9
a|
Controller B, port 2
a|
 
a|
7
a|
Host 1, port 2
a|
 
a|
 
a|
(Optional) Host 2, port 2
a|
 
|===

=== NVMe over InfiniBand: Mapping host name

NOTE: The mapping host name is created during the workflow.

|===
a|
Mapping host name
a|
 
a|
Host OS type
a|
 
|===
