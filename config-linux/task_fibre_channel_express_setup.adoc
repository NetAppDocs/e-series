---
permalink: config-linux/task_fibre_channel_express_setup.html
sidebar: sidebar
keywords: 
summary: ''
---
= Fibre Channel Express Setup
:experimental:
:icons: font
:imagesdir: ../media/

[.lead]
== Verify the Linux configuration is supported

[.lead]
To ensure reliable operation, you create an implementation plan and then use the NetApp Interoperability Matrix Tool (IMT) to verify that the entire configuration is supported.

. Go to the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].
. Click on the *Solution Search* tile.
. In the menu:Protocols[SAN Host] area, click the *Add* button next to *E-Series SAN Host*.
. Click *View Refine Search Criteria*.
+
The *Refine Search Criteria* section is displayed. In this section you may select the protocol that applies, as well as other criteria for the configuration such as Operating System, NetApp OS, and Host Multipath driver. Select the criteria you know you want for your configuration, and then see what compatible configuration elements apply. As necessary, make the updates for your operating system and protocol that are prescribed in the tool. Detailed information for your chosen configuration is accessible on the View Supported Configurations page by clicking the *right page arrow*.

== Configure IP addresses using DHCP

[.lead]
In this express method for configuring communications between the management station and the storage array, you use Dynamic Host Configuration Protocol (DHCP) to provide IP addresses. Each storage array has either one controller (simplex) or two controllers (duplex), and each controller has two storage management ports. Each management port will be assigned an IP address.

You have installed and configured a DHCP server on the same subnet as the storage management ports.

The following instructions refer to a storage array with two controllers (a duplex configuration).

. If you have not already done so, connect an Ethernet cable to the management station and to management port 1 on each controller (A and B).
+
The DHCP server assigns an IP address to port 1 of each controller.
+
NOTE: Do not use management port 2 on either controller. Port 2 is reserved for use by NetApp technical personnel.
+
IMPORTANT: If you disconnect and reconnect the Ethernet cable, or if the storage array is power-cycled, DHCP assigns IP addresses again. This process occurs until static IP addresses are configured. It is recommended that your avoid disconnecting the cable or power-cycling the array.
+
If the storage array cannot get DHCP-assigned IP addresses within 30 seconds, the following default IP addresses are set:

 ** Controller A, port 1: 169.254.128.101
 ** Controller B, port 1: 169.254.128.102
 ** Subnet mask: 255.255.0.0

. Locate the MAC address label on the back of each controller, and then provide your network administrator with the MAC address for port 1 of each controller.
+
Your network administrator needs the MAC addresses to determine the IP address for each controller. You will need the IP addresses to connect to your storage system through your browser.

== Install and configure Linux Unified Host Utilities

[.lead]
Linux Unified Host Utilities 7.1 includes tools to manage NetApp storage, including failover policies and physical paths.

. Use the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] to determine the appropriate version of Unified Host Utilities 7.1 to install.
+
The versions are listed in a column within each supported configuration.

. Download the Unified Host Utilities 7.1 from https://mysupport.netapp.com/site/[NetApp Support].
+
NOTE: Alternatively, you can use the SANtricity `SMdevices` utility to perform the same functions as the Unified Host Utility tool. The `SMdevices` utility is included as part of the `SMutils` package. The `SMutils` package is a collection of utilities to verify what the host sees from the storage array. It is included as part of the SANtricity software installation.

== Install SANtricity Storage Manager for SMcli (SANtricity software version 11.53 or earlier)

[.lead]
When you install the SANtricity Storage Manager software on your management station, the command line interface (CLI) is installed to help you manage your array. By also installing the software on the host, the Host Context Agent is installed that helps the host push configuration information to the storage array controllers through the I/O path.

IMPORTANT: For SANtricity software 11.60 and newer, the SANtricity Secure CLI (SMcli) is included in the SANtricity OS and downloadable through the SANtricity System Manager. For more information on how to download the SMcli through the SANtricty System Manager, refer to the _Download command line interface (CLI)_ topic under the SANtricity System Manager Online Help.

* You are using SANtricity software 11.53 or earlier.
* You must have the correct administrator or superuser privileges.
* You must have ensured that the system that will contain the SANtricity Storage Manager client has the following minimum requirements:
 ** *RAM*: 2 GB for Java Runtime Engine
 ** *Disk space*: 5 GB
 ** *OS/Architecture*: Refer to https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager* for guidance on determining the supported operating system versions and architectures.

This section describes how to install SANtricity Storage Manager on both the Windows and Linux OS platforms, because both Windows and Linux are common management station platforms when Linux is used for the data host.

. Download the SANtricity software release from https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager*.
. Run the SANtricity installer.
+
|===
| Windows| Linux
a|
Double-click the SMIA*.exe installation package to start the installation.
a|

 .. Go to the directory where the SMIA*.bin installation package is located.
 .. If the temp mount point does not have execute permissions, set the IATEMPDIR variable. Example: IATEMPDIR=/root ./SMIA-LINUXX64-11.25.0A00.0002.bin
 .. Run the chmod +x SMIA*.bin command to grant execute permission to the file.
 .. Run the ./SMIA*.bin command to start the installer.

+
|===

. Use the installation wizard to install the software on the management station.

== Access SANtricity System Manager and use the Setup wizard

[.lead]
You use the Setup wizard in SANtricity System Manager to configure your storage array.

* You have ensured that the device from which you will access SANtricity System Manager contains one of the following browsers:
+
|===
| Browser| Minimum version
a|
Google Chrome
a|
47
a|
Microsoft Internet Explorer
a|
11
a|
Microsoft Edge
a|
EdgeHTML 12
a|
Mozilla Firefox
a|
31
a|
Safari
a|
9
|===

* You are using out-of-band management.

The wizard automatically relaunches when you open System Manager or refresh your browser and _at least one_ of the following conditions is met:

* No pools and volume groups are detected.
* No workloads are detected.
* No notifications are configured.

. From your browser, enter the following URL: `https://<DomainNameOrIPAddress>`
+
IPAddress is the address for one of the storage array controllers.
+
The first time SANtricity System Manager is opened on an array that has not been configured, the Set Administrator Password prompt appears. Role-based access management configures four local roles: admin, support, security, and monitor. The latter three roles have random passwords that cannot be guessed. After you set a password for the admin role you can change all of the passwords using the admin credentials. See _SANtricity System Manager online help_ for more information on the four local user roles.

. Enter the System Manager password for the admin role in the Set Administrator Password and Confirm Password fields, and then select the *Set Password* button.
+
When you open System Manager and no pools, volumes groups, workloads, or notifications have been configured, the Setup wizard launches.

. Use the Setup wizard to perform the following tasks:
 ** *Verify hardware (controllers and drives)* -- Verify the number of controllers and drives in the storage array. Assign a name to the array.
 ** *Verify hosts and operating systems* -- Verify the host and operating system types that the storage array can access.
 ** *Accept pools* -- Accept the recommended pool configuration for the express installation method. A pool is a logical group of drives.
 ** *Configure alerts* -- Allow System Manager to receive automatic notifications when a problem occurs with the storage array.
 ** *Enable AutoSupport* -- Automatically monitor the health of your storage array and have dispatches sent to technical support.
. If you have not already created a volume, create one by going to *Storage* > *Volumes* > *Create* > *Volume*.
+
For more information, see the online help for SANtricity System Manager.

== Configure the multipath software

[.lead]
Multipath software provides a redundant path to the storage array in case one of the physical paths is disrupted. The multipath software presents the operating system with a single virtual device that represents the active physical paths to the storage. The multipath software also manages the failover process that updates the virtual device. You use the device mapper multipath (DM-MP) tool for Linux installations.

You have installed the required packages on your system.

* For Red Hat (RHEL) hosts, verify the packages are installed by running rpm -q device-mapper-multipath.
* For SLES hosts, verify the packages are installed by running rpm -q multipath-tools.

By default, DM-MP is disabled in RHEL and SLES. Complete the following steps to enable DM-MP components on the host.

If you have not already installed the operating system, use the media supplied by your operating system vendor.

. If a multipath.conf file is not already created, run the # touch /etc/multipath.conf command.
. Use the default multipath settings by leaving the multipath.conf file blank.
. Start the multipath service.
+
----
# systemctl start multipathd
----

. Save your kernel version by running the uname -r command.
+
----
# uname -r
3.10.0-327.el7.x86_64
----
+
You will use this information when you assign volumes to the host.

. Do one of the following to enable the multipathd daemon on boot.
+
|===
| If you are using....| Do this...
a|
RHEL 7.x and 8.x systems:
a|
systemctl enable multipathd
a|
SLES 12.x and 15.x systems:
a|
systemctl enable multipathd
|===

. Rebuild the initramfs image or the initrd image under /boot directory:
+
|===
| If you are using....| Do this...
a|
RHEL 7.x and 8.x systems:
a|
dracut --force --add multipath
a|
SLES 12.x and 15.x systems:
a|
dracut --force --add multipath
|===

. Make sure that the newly created /boot/initrams-* image or /boot/initrd-* image is selected in the boot configuration file.
+
For example, for grub it is /boot/grub/menu.lst and for grub2 it is /boot/grub2/menu.cfg.

. Use the "Create host manually" procedure in the online help to check whether the hosts are defined. Verify that each host type is either *Linux DM-MP (Kernel 3.10 or later)* if you enable the Automatic Load Balancing feature, or *Linux DM-MP (Kernel 3.9 or earlier)* if you disable the Automatic Load Balancing feature. If necessary, change the selected host type to the appropriate setting.
. Reboot the host.

== Setting up the multipath.conf file

[.lead]
The multipath.conf file is the configuration file for the multipath daemon, multipathd. The multipath.conf file overrides the built-in configuration table for multipathd. Any line in the file whose first non-white-space character is # is considered a comment line. Empty lines are ignored.

NOTE: For SANtricity operating system 8.30 and newer, NetApp recommends using the default settings as provided.

The multipath.conf are available in the following locations:

* For SLES, /usr/share/doc/packages/multipath-tools/multipath.conf.synthetic
* For RHEL, /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf

== Configure the FC switches

[.lead]
Configuring (zoning) the Fibre Channel (FC) switches enables the hosts to connect to the storage array and limits the number of paths. You zone the switches using the management interface for the switches.

* You must have administrator credentials for the switches.
* You must have used your HBA utility to discover the WWPN of each host initiator port and of each controller target port connected to the switch.

For details about zoning your switches, see the switch vendor's documentation.

Each initiator port must be in a separate zone with all of its corresponding target ports.

. Log in to the FC switch administration program, and then select the zoning configuration option.
. Create a new zone that includes the first host initiator port and that also includes all of the target ports that connect to the same FC switch as the initiator.
. Create additional zones for each FC host initiator port in the switch.
. Save the zones, and then activate the new zoning configuration.

== Determine host WWPNs and make the recommended settings

[.lead]
You install an FC HBA utility so you can view the worldwide port name (WWPN) of each host port. Additionally, you can use the HBA utility to change any settings recommended in the Notes column of the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] for the supported configuration.

Guidelines for HBA utilities:

* Most HBA vendors offer an HBA utility. You will need the correct version of HBA for your host operating system and CPU. Examples of FC HBA utilities include:
 ** Emulex OneCommand Manager for Emulex HBAs
 ** QLogic QConverge Console for QLogic HBAs
* Host I/O ports might automatically register if the host context agent is installed.

. Download the appropriate utility from your HBA vendor's web site.
. Install the utility.
. Select the appropriate settings in the HBA utility.
+
Appropriate settings for your configuration are listed in the Notes column of the IMT.

== Create partitions and filesystems

[.lead]
A new LUN has no partition or file system when the Linux host first discovers it. You must format the LUN before it can be used. Optionally, you can create a file system on the LUN.

The host must have discovered the LUN.

In the /dev/mapper folder, you have run the ls command to see the available disks.

You can initialize the disk as a basic disk with a GUID partition table (GPT) or Master boot record (MBR).

Format the LUN with a file system such as ext4. Some applications do not require this step.

. Retrieve the SCSI ID of the mapped disk by issuing the sanlun lun show -p command.
+
The SCSI ID is a 33-character string of hexadecimal digits, beginning with the number 3. If user-friendly names are enabled, Device Mapper reports disks as mpath instead of by a SCSI ID.
+
----
# sanlun lun show -p

                E-Series Array: ictm1619s01c01-SRP(60080e50002908b40000000054efb9d2)
                   Volume Name:
               Preferred Owner: Controller in Slot B
                 Current Owner: Controller in Slot B
                          Mode: RDAC (Active/Active)
                       UTM LUN: None
                           LUN: 116
                      LUN Size:
                       Product: E-Series
                   Host Device: mpathr(360080e50004300ac000007575568851d)
              Multipath Policy: round-robin 0
            Multipath Provider: Native
--------- ---------- ------- ------------ ----------------------------------------------
host      controller                      controller
path      path       /dev/   host         target
state     type       node    adapter      port
--------- ---------- ------- ------------ ----------------------------------------------
up        secondary  sdcx    host14       A1
up        secondary  sdat    host10       A2
up        secondary  sdbv    host13       B1
----

. Create a new partition according to the method appropriate for your Linux OS release.
+
Typically, characters identifying the partition of a disk are appended to the SCSI ID (the number 1 or p3 for instance).
+
----
# parted -a optimal -s -- /dev/mapper/360080e5000321bb8000092b1535f887a mklabel
gpt mkpart primary ext4 0% 100%
----

. Create a file system on the partition.
+
The method for creating a file system varies depending on the file system chosen.
+
----
# mkfs.ext4 /dev/mapper/360080e5000321bb8000092b1535f887a1
----

. Create a folder to mount the new partition.
+
----
# mkdir /mnt/ext4
----

. Mount the partition.
+
----
# mount /dev/mapper/360080e5000321bb8000092b1535f887a1 /mnt/ext4
----

== Verify storage access on the host

[.lead]
Before using the volume, you verify that the host can write data to the volume and read it back.

You must have initialized the volume and formatted it with a file system.

. On the host, copy one or more files to the mount point of the disk.
. Copy the files back to a different folder on the original disk.
. Run the diff command to compare the copied files to the originals.

Remove the file and folder that you copied.

== FC worksheet for Linux

[.lead]
You can use this worksheet to record FC storage configuration information. You need this information to perform provisioning tasks.

The illustration shows a host connected to an E-Series storage array in two zones. One zone is indicated by the blue line; the other zone is indicated by the red line. Any single port has two paths to the storage (one to each controller).

image::../media/port_identifiers_host_and_target.gif[]

=== Host identifiers

|===
| Callout No.| Host (initiator) port connections| WWPN
a|
1
a|
Host
a|
_not applicable_
a|
2
a|
Host port 0 to FC switch zone 0
a|
 
a|
7
a|
Host port 1 to FC switch zone 1
a|
 
|===

=== Target identifiers

|===
| Callout No.| Array controller (target) port connections| WWPN
a|
3
a|
Switch
a|
_not applicable_
a|
6
a|
Array controller (target)
a|
_not applicable_
a|
5
a|
Controller A, port 1 to FC switch 1
a|
 
a|
9
a|
Controller A, port 2 to FC switch 2
a|
 
a|
4
a|
Controller B, port 1 to FC switch 1
a|
 
a|
8
a|
Controller B, port 2 to FC switch 2
a|
 
|===

=== Mapping host

|===
a|
Mapping host name
a|
 
a|
Host OS type
a|
 
|===
