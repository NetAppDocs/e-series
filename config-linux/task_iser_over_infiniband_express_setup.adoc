---
permalink: config-linux/task_iser_over_infiniband_express_setup.html
sidebar: sidebar
keywords: 
summary: ''
---
= iSER over InfiniBand Express Setup
:experimental:
:icons: font
:imagesdir: ../media/

[.lead]
== Verify the Linux configuration is supported

[.lead]
To ensure reliable operation, you create an implementation plan and then use the NetApp Interoperability Matrix Tool (IMT) to verify that the entire configuration is supported.

. Go to the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].
. Click on the *Solution Search* tile.
. In the menu:Protocols[SAN Host] area, click the *Add* button next to *E-Series SAN Host*.
. Click *View Refine Search Criteria*.
+
The *Refine Search Criteria* section is displayed. In this section you may select the protocol that applies, as well as other criteria for the configuration such as Operating System, NetApp OS, and Host Multipath driver. Select the criteria you know you want for your configuration, and then see what compatible configuration elements apply. As necessary, make the updates for your operating system and protocol that are prescribed in the tool. Detailed information for your chosen configuration is accessible on the View Supported Configurations page by clicking the *right page arrow*.

== Configure IP addresses using DHCP

[.lead]
In this express method for configuring communications between the management station and the storage array, you use Dynamic Host Configuration Protocol (DHCP) to provide IP addresses. Each storage array has either one controller (simplex) or two controllers (duplex), and each controller has two storage management ports. Each management port will be assigned an IP address.

You have installed and configured a DHCP server on the same subnet as the storage management ports.

The following instructions refer to a storage array with two controllers (a duplex configuration).

. If you have not already done so, connect an Ethernet cable to the management station and to management port 1 on each controller (A and B).
+
The DHCP server assigns an IP address to port 1 of each controller.
+
NOTE: Do not use management port 2 on either controller. Port 2 is reserved for use by NetApp technical personnel.
+
IMPORTANT: If you disconnect and reconnect the Ethernet cable, or if the storage array is power-cycled, DHCP assigns IP addresses again. This process occurs until static IP addresses are configured. It is recommended that your avoid disconnecting the cable or power-cycling the array.
+
If the storage array cannot get DHCP-assigned IP addresses within 30 seconds, the following default IP addresses are set:

 ** Controller A, port 1: 169.254.128.101
 ** Controller B, port 1: 169.254.128.102
 ** Subnet mask: 255.255.0.0

. Locate the MAC address label on the back of each controller, and then provide your network administrator with the MAC address for port 1 of each controller.
+
Your network administrator needs the MAC addresses to determine the IP address for each controller. You will need the IP addresses to connect to your storage system through your browser.

== Configure subnet manager

[.lead]
Using an InfiniBand switch to run subnet manager might cause unexpected path loss during high loads. To avoid path loss, configure the subnet manager on one or more of your hosts using opensm.

. Install the opensm package on any hosts that will be running the subnet manager.
. Start and enable the opensm service.
. Use the ``ibstat -p``command to find `GUID0` and `GUID1` of the HBA ports. For example:
+
----
# ibstat -p
 0x248a070300a80a80
 0x248a070300a80a81
----

. Start two instances of the subnet manager, one for each subnet, by adding the following commands to `/etc/rc.d/after.local` for `SUSE` or `etc/rc.d/rc.local` for `Redhat`. Substitute the values you found in the last step for `GUID0` and `GUID1`. For `P0` and `P1`, use the subnet manager priorities, with 1 being the lowest and 15 the highest:
+
----
 opensm -B -g GUID0 -p P0 -f /var/log/opensm-ib0.log
 opensm -B -g GUID1 -p P1 -f /var/log/opensm-ib1.log
----

== Install and configure Host Utilities

[.lead]
Linux Unified Host Utilities 7.1 includes tools to manage NetApp storage, including failover policies and physical paths.

. Use the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool] to determine the appropriate version of Unified Host Utilities 7.1 to install.
+
The versions are listed in a column within each supported configuration.

. Download the Unified Host Utilities 7.1 from https://mysupport.netapp.com/site/[NetApp Support].
+
NOTE: Alternatively, you can use the SANtricity `SMdevices` utility to perform the same functions as the Unified Host Utility tool. The `SMdevices` utility is included as part of the `SMutils` package. The `SMutils` package is a collection of utilities to verify what the host sees from the storage array. It is included as part of the SANtricity software installation.

== Install SANtricity Storage Manager for SMcli (SANtricity software version 11.53 or earlier)

[.lead]
When you install the SANtricity Storage Manager software on your management station, a graphical user interface (GUI) and a command line interface (CLI) are installed by default. These instructions assume that you will install the SANtricity Storage Manager GUI on a management station and _not_ on the I/O host.

IMPORTANT: For SANtricity software 11.60 and newer, the SANtricity Secure CLI (SMcli) is included in the SANtricity OS and downloadable through the SANtricity System Manager. For more information on how to download the SMcli through the SANtricty System Manager, refer to the _Download the command line interface (CLI)_ topic under the SANtricity System Manager Online Help.

* You are using SANtricity software 11.53 or earlier.
* You must have the correct administrator or superuser privileges.
* You must have ensured that the system that will contain the SANtricity Storage Manager client has the following minimum requirements:
 ** *RAM*: 2 GB for Java Runtime Engine
 ** *Disk space*: 5 GB
 ** *OS/Architecture*: Refer to https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager* for guidance on determining the supported operating system versions and architectures.

This section describes how to install SANtricity Storage Manager on both the Windows and Linux OS platforms, because both Windows and Linux are common management station platforms when Linux is used for the data host.

. Download the SANtricity software release from https://mysupport.netapp.com/site/[NetApp Support]*Downloads* > *Software* > *E-Series/EF-Series SANtricity Storage Manager*.
. Run the SANtricity installer.
+
|===
| Windows| Linux
a|
Double-click the SMIA*.exe installation package to start the installation.
a|

 .. Go to the directory where the SMIA*.bin installation package is located.
 .. If the temp mount point does not have execute permissions, set the IATEMPDIR variable. Example: IATEMPDIR=/root ./SMIA-LINUXX64-11.25.0A00.0002.bin
 .. Run the chmod +x SMIA*.bin command to grant execute permission to the file.
 .. Run the ./SMIA*.bin command to start the installer.

+
|===

. Use the installation wizard to install the software on the management station.

== Access SANtricity System Manager and use the Setup wizard

[.lead]
You use the Setup wizard in SANtricity System Manager to configure your storage array.

* You have ensured that the device from which you will access SANtricity System Manager contains one of the following browsers:
+
|===
| Browser| Minimum version
a|
Google Chrome
a|
47
a|
Microsoft Internet Explorer
a|
11
a|
Microsoft Edge
a|
EdgeHTML 12
a|
Mozilla Firefox
a|
31
a|
Safari
a|
9
|===

* You are using out-of-band management.

The wizard automatically relaunches when you open System Manager or refresh your browser and _at least one_ of the following conditions is met:

* No pools and volume groups are detected.
* No workloads are detected.
* No notifications are configured.

. From your browser, enter the following URL: `https://<DomainNameOrIPAddress>`
+
IPAddress is the address for one of the storage array controllers.
+
The first time SANtricity System Manager is opened on an array that has not been configured, the Set Administrator Password prompt appears. Role-based access management configures four local roles: admin, support, security, and monitor. The latter three roles have random passwords that cannot be guessed. After you set a password for the admin role you can change all of the passwords using the admin credentials. See _SANtricity System Manager online help_ for more information on the four local user roles.

. Enter the System Manager password for the admin role in the Set Administrator Password and Confirm Password fields, and then select the *Set Password* button.
+
When you open System Manager and no pools, volumes groups, workloads, or notifications have been configured, the Setup wizard launches.

. Use the Setup wizard to perform the following tasks:
 ** *Verify hardware (controllers and drives)* -- Verify the number of controllers and drives in the storage array. Assign a name to the array.
 ** *Verify hosts and operating systems* -- Verify the host and operating system types that the storage array can access.
 ** *Accept pools* -- Accept the recommended pool configuration for the express installation method. A pool is a logical group of drives.
 ** *Configure alerts* -- Allow System Manager to receive automatic notifications when a problem occurs with the storage array.
 ** *Enable AutoSupport* -- Automatically monitor the health of your storage array and have dispatches sent to technical support.
. If you have not already created a volume, create one by going to *Storage* > *Volumes* > *Create* > *Volume*.
+
For more information, see the online help for SANtricity System Manager.

== Configure the multipath software

[.lead]
Multipath software provides a redundant path to the storage array in case one of the physical paths is disrupted. The multipath software presents the operating system with a single virtual device that represents the active physical paths to the storage. The multipath software also manages the failover process that updates the virtual device. You use the device mapper multipath (DM-MP) tool for Linux installations.

You have installed the required packages on your system.

* For Red Hat (RHEL) hosts, verify the packages are installed by running rpm -q device-mapper-multipath.
* For SLES hosts, verify the packages are installed by running rpm -q multipath-tools.

By default, DM-MP is disabled in RHEL and SLES. Complete the following steps to enable DM-MP components on the host.

If you have not already installed the operating system, use the media supplied by your operating system vendor.

. If a multipath.conf file is not already created, run the # touch /etc/multipath.conf command.
. Use the default multipath settings by leaving the multipath.conf file blank.
. Start the multipath service.
+
----
# systemctl start multipathd
----

. Save your kernel version by running the uname -r command.
+
----
# uname -r
3.10.0-327.el7.x86_64
----
+
You will use this information when you assign volumes to the host.

. Do one of the following to enable the multipathd daemon on boot.
+
|===
| If you are using....| Do this...
a|
RHEL 7.x and 8.x systems:
a|
systemctl enable multipathd
a|
SLES 12.x and 15.x systems:
a|
systemctl enable multipathd
|===

. Rebuild the initramfs image or the initrd image under /boot directory:
+
|===
| If you are using....| Do this...
a|
RHEL 7.x and 8.x systems:
a|
dracut --force --add multipath
a|
SLES 12.x and 15.x systems:
a|
dracut --force --add multipath
|===

. Make sure that the newly created /boot/initrams-* image or /boot/initrd-* image is selected in the boot configuration file.
+
For example, for grub it is /boot/grub/menu.lst and for grub2 it is /boot/grub2/menu.cfg.

. Use the "Create host manually" procedure in the online help to check whether the hosts are defined. Verify that each host type is either *Linux DM-MP (Kernel 3.10 or later)* if you enable the Automatic Load Balancing feature, or *Linux DM-MP (Kernel 3.9 or earlier)* if you disable the Automatic Load Balancing feature. If necessary, change the selected host type to the appropriate setting.
. Reboot the host.

== Setting up the multipath.conf file

[.lead]
The multipath.conf file is the configuration file for the multipath daemon, multipathd. The multipath.conf file overrides the built-in configuration table for multipathd. Any line in the file whose first non-white-space character is # is considered a comment line. Empty lines are ignored.

NOTE: For SANtricity operating system 8.30 and newer, NetApp recommends using the default settings as provided.

The multipath.conf are available in the following locations:

* For SLES, /usr/share/doc/packages/multipath-tools/multipath.conf.synthetic
* For RHEL, /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf

== Configure network connections--iSER over Infiniband

[.lead]
If your configuration uses the iSER over Infiniband protocol, perform the steps in this section.

When you are using a 56-Gbps HIC with the iSER over Infiniband protocol, additional array configuration is required.

. From the *Setup* tab, select *Configure iSCSI Host Ports* to set the storage array iSCSI addresses.
+
Put the array iSCSI addresses on the same subnet as the host port(s) you will use to create iSCSI sessions. For addresses, see link:task_iser_over_infiniband_express_setup.md#[iSER worksheet].

. From the *Devices* tab, select the storage array and go to menu:iSER[Manage Settings] to find the IQN.
+
This information might be necessary when you create iSER sessions from operating systems that do not support send targets discovery. Enter this information in the worksheet, in link:task_iser_over_infiniband_express_setup.md#[iSER worksheet].

== Configure networking for storage attached hosts--iSER over Infiniband

[.lead]
The InfiniBand OFED driver stack supports running both iSER and SRP simultaneously on the same ports, so no additional hardware is required.

A NetApp recommended OFED is installed on the system. For more information, see the https://mysupport.netapp.com/matrix[NetApp Interoperability Matrix Tool].

. Enable and start iSCSI services on the host(s):
+
*Red Hat Enterprise Linux 7 and 8 (RHEL 7 and RHEL 8)*
+
----

# systemctl start iscsi
# systemctl start iscsid
# systemctl enable iscsi
# systemctl enable iscsid
----
+
*SUSE Linux Enterprise Server 12 and 15 (SLES 12 and SLES 15)*
+
----

# systemctl start iscsid.service
# systemctl enable iscsid.service
----

. Configure IPoIB network interfaces:
 .. Identify the InfiniBand ports that will be used. Document the HW Address (MAC address) of each port.
 .. Configure persistent names for the InfiniBand network interface devices.
 .. Configure the IP address and network information for the IPoIB interfaces identified.
+
The specific interface configuration required might vary depending on the operating system used. Consult your vendor's operating system documentation for specific information on implementation.

 .. Start the IB network interfaces by restarting the networking service or by manually restarting each interface. For example:
+
----
systemctl restart network
----

 .. Verify connectivity to the target ports. From the host, ping the IP addresses you configured when you configured network connections.
. Restart services to load the iSER module.
. Edit the iSCSI settings in /etc/iscsi/iscsid.conf.
+
----
node.startup = automatic replacement_timeout = 20
----

. Create iSCSI session configurations:
 .. Create iface configuration files for each InfiniBand interface.
+
NOTE: The directory location for the iSCSI iface files is operating system dependent. This example is for using Red Hat Enterprise Linux:
+
----
iscsiadm -m iface -I iser > /var/lib/iscsi/ifaces/iface-ib0
iscsiadm -m iface -I iser > /var/lib/iscsi/ifaces/iface-ib1
----

 .. Edit each iface file to set the interface name and initiator IQN. Set the following parameters appropriately for each iface file:
+
|===
| Option| Value
a|
iface.net_ifacename
a|
The interface device name (ex. ib0).
a|
iface.initiatorname
a|
The host initiator IQN documented in the worksheet.
|===

 .. Create iSCSI sessions to the target.
+
The preferred method to create the sessions is to use the SendTargets discovery method. However, this method does not work on some operating system releases.
+
NOTE: Use *Method 2* for RHEL 6.x or SLES 11.3 or later.

 ** *Method 1 - SendTargets discovery:* Use the SendTargets discovery mechanism to one of the target portal IP addresses. This will create sessions for each of the target portals.
+
----
iscsiadm -m discovery -t st -p 192.168.130.101 -I iser
----

 ** *Method 2 - Manual creation:* For each target portal IP address, create a session using the appropriate host interface iface configuration. In this example, interface ib0 is on subnet A and interface ib1 is on subnet B. For these variables, substitute the appropriate value from the worksheet:
  *** <Target IQN> = storage array Target IQN
  *** <Target Port IP> = IP address configured on the specified target port

+
----
# Controller A Port 1
iscsiadm -m node -target <Target IQN\> -I iface-ib0 -p <Target Port IP\> -l -o new
# Controller B Port 1
iscsiadm -m node -target <Target IQN\> -I iface-ib0 -p <Target Port IP\> -l -o new
# Controller A Port 2
iscsiadm -m node -target <Target IQN\> -I iface-ib1 -p <Target Port IP\> -l -o new
# Controller B Port 2
iscsiadm -m node -target <Target IQN\> -I iface-ib1 -p <Target Port IP\> -l -o new
----
. Log in to iSCSI sessions.
+
For each session, run the iscsiadm command to log in to the session.
+
----
# Controller A Port 1
iscsiadm -m node -target <Target IQN\> -I iface -ib0 -p <Target Port IP\> -l
# Controller B Port 1
iscsiadm -m node -target <Target IQN\> -I iface -ib0 -p <Target Port IP\> -l
# Controller A Port 2
iscsiadm -m node -target <Target IQN\> -I iface -ib1 -p <Target Port IP\> -l
# Controller B Port 2
iscsiadm -m node -target <Target IQN\> -I iface -ib1 -p <Target Port IP\> -l
----

. Verify the iSER/iSCSI sessions.
 .. Check the iscsi session status from the host:
+
----
iscsiadm -m session
----

 .. Check the iscsi session status from the array. From SANtricity System Manager, navigate to the *Storage Array* > *iSER* > *View/End Sessions*.

When the OFED/RDMA service starts, the iSER kernel module(s) loads by default when the iSCSI services are running. To complete the iSER connection setup, the iSER module(s) should be loaded. Currently this requires a host reboot.

== Create partitions and filesystems

[.lead]
A new LUN has no partition or file system when the Linux host first discovers it. You must format the LUN before it can be used. Optionally, you can create a file system on the LUN.

The host must have discovered the LUN.

In the /dev/mapper folder, you have run the ls command to see the available disks.

You can initialize the disk as a basic disk with a GUID partition table (GPT) or Master boot record (MBR).

Format the LUN with a file system such as ext4. Some applications do not require this step.

. Retrieve the SCSI ID of the mapped disk by issuing the sanlun lun show -p command.
+
The SCSI ID is a 33-character string of hexadecimal digits, beginning with the number 3. If user-friendly names are enabled, Device Mapper reports disks as mpath instead of by a SCSI ID.
+
----
# sanlun lun show -p

                E-Series Array: ictm1619s01c01-SRP(60080e50002908b40000000054efb9d2)
                   Volume Name:
               Preferred Owner: Controller in Slot B
                 Current Owner: Controller in Slot B
                          Mode: RDAC (Active/Active)
                       UTM LUN: None
                           LUN: 116
                      LUN Size:
                       Product: E-Series
                   Host Device: mpathr(360080e50004300ac000007575568851d)
              Multipath Policy: round-robin 0
            Multipath Provider: Native
--------- ---------- ------- ------------ ----------------------------------------------
host      controller                      controller
path      path       /dev/   host         target
state     type       node    adapter      port
--------- ---------- ------- ------------ ----------------------------------------------
up        secondary  sdcx    host14       A1
up        secondary  sdat    host10       A2
up        secondary  sdbv    host13       B1
----

. Create a new partition according to the method appropriate for your Linux OS release.
+
Typically, characters identifying the partition of a disk are appended to the SCSI ID (the number 1 or p3 for instance).
+
----
# parted -a optimal -s -- /dev/mapper/360080e5000321bb8000092b1535f887a mklabel
gpt mkpart primary ext4 0% 100%
----

. Create a file system on the partition.
+
The method for creating a file system varies depending on the file system chosen.
+
----
# mkfs.ext4 /dev/mapper/360080e5000321bb8000092b1535f887a1
----

. Create a folder to mount the new partition.
+
----
# mkdir /mnt/ext4
----

. Mount the partition.
+
----
# mount /dev/mapper/360080e5000321bb8000092b1535f887a1 /mnt/ext4
----

== Verify storage access on the host

[.lead]
Before using the volume, you verify that the host can write data to the volume and read it back.

You must have initialized the volume and formatted it with a file system.

. On the host, copy one or more files to the mount point of the disk.
. Copy the files back to a different folder on the original disk.
. Run the diff command to compare the copied files to the originals.

Remove the file and folder that you copied.

== iSER over InfiniBand worksheet

[.lead]
You can use this worksheet to record iSER over Infiniband storage configuration information. You need this information to perform provisioning tasks.

=== iSER over InfiniBand: Host identifiers

NOTE: The software initiator IQN is determined during the task, link:task_iser_over_infiniband_express_setup.md#[Configuring storage attached hosts with iSER networking].

Locate and document the initiator IQN from each host. For software initiators, the IQN is typically found in the /etc/iscsi/initiatorname.iscsi file.

|===
| Callout No.| Host port connections| Software initiator IQN
a|
1
a|
Host (initiator) 1
a|
 
a|
n/a
a|
 
a|
 
a|
n/a
a|
 
a|
 
a|
n/a
a|
 
a|
 
a|
n/a
a|
 
a|
 
|===

=== iSER over InfiniBand: Recommended configuration

Recommended configurations consist of two host (initiator) ports and four target ports.

image::../media/port_identifiers_ib_iser.gif[]

=== iSER over InfiniBand: Target IQN

Document the target IQN for the storage array. You will use this information in link:task_iser_over_infiniband_express_setup.md#[Configuring storage attached hosts with iSER networking].

Find the Storage Array IQN name using SANtricity: *Storage Array* > *iSER* > *Manage Settings*. This information might be necessary when you create iSER sessions from operating systems that do not support send targets discovery.

|===
| Callout No.| Array name| Target IQN
a|
6
a|
Array controller (target)
a|
 
|===

=== iSER over InfiniBand: Network configuration

Document the network configuration that will be used for the hosts and storage on the InfiniBand fabric. These instructions assume that two subnets will be used for full redundancy.

Your network administrator can provide the following information. You use this information in the topic, link:task_iser_over_infiniband_express_setup.md#[Configuring storage attached hosts with iSER networking].

=== Subnet A

Define the subnet to be used.

|===
| Network Address| Netmask
a|
 
a|
 
|===
Document the IQNs to be used by the array ports and each host port.

|===
| Callout No.| Array controller (target) port connections| IQN
a|
3
a|
Switch
a|
_not applicable_
a|
5
a|
Controller A, port 1
a|
 
a|
4
a|
Controller B, port 1
a|
 
a|
2
a|
Host 1, port 1
a|
 
a|
 
a|
(Optional) Host 2, port 1
a|
 
|===

=== Subnet B

Define the subnet to be used.

|===
| Network Address| Netmask
a|
 
a|
 
|===
Document the IQNs to be used by the array ports and each host port.

|===
| Callout No.| Array controller (target) port connections| IQN
a|
8
a|
Switch
a|
_not applicable_
a|
10
a|
Controller A, port 2
a|
 
a|
9
a|
Controller B, port 2
a|
 
a|
7
a|
Host 1, port 2
a|
 
a|
 
a|
(Optional) Host 2, port 2
a|
 
|===

=== iSER over InfiniBand: Mapping host name

NOTE: The mapping host name is created during the workflow.

|===
a|
Mapping host name
a|
 
a|
Host OS type
a|
 
|===
